{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKidl6Y56VRA"
      },
      "source": [
        "### Model with updated loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFwu5gFzYtNi"
      },
      "source": [
        "The 1989-era model had a slightly strange (by our \"modern\" standard) output layer, for a multi-class classification problem - \n",
        "\n",
        "* there is a tanh activation on the output units, which maps the output for each of the 10 outputs to the range -1 to 1.\n",
        "* and then there is a mean squared error loss function on the outputs.\n",
        "\n",
        "We will \"update\" the model by removing the tanh activation on the output units, and then using the typical (for a classification problem) [cross entropy loss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss).\n",
        "\n",
        "I also noticed that when I ran this modified model, the loss seemed to \"blow up\" suggesting a too-high learning rate - so I reduced the learning rate from 0.03 to 0.01."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJqRxqAdelLg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "torch.set_num_threads(2) # for performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SlWnqk_t6bEX"
      },
      "outputs": [],
      "source": [
        "\n",
        "class ModernLossNet(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # initialization as described in the paper to my best ability, but it doesn't look right...\n",
        "        winit = lambda fan_in, *shape: (torch.rand(*shape) - 0.5) * 2 * 2.4 / fan_in**0.5\n",
        "        macs = 0 # keep track of MACs (multiply accumulates)\n",
        "        acts = 0 # keep track of number of activations\n",
        "\n",
        "        # H1 layer parameters and their initialization\n",
        "        self.H1w = nn.Parameter(winit(5*5*1, 12, 1, 5, 5))\n",
        "        self.H1b = nn.Parameter(torch.zeros(12, 8, 8)) # presumably init to zero for biases\n",
        "        assert self.H1w.nelement() + self.H1b.nelement() == 1068\n",
        "        macs += (5*5*1) * (8*8) * 12\n",
        "        acts += (8*8) * 12\n",
        "\n",
        "        # H2 layer parameters and their initialization\n",
        "        \"\"\"\n",
        "        H2 neurons all connect to only 8 of the 12 input planes, with an unspecified pattern\n",
        "        I am going to assume the most sensible block pattern where 4 planes at a time connect\n",
        "        to differently overlapping groups of 8/12 input planes. We will implement this with 3\n",
        "        separate convolutions that we concatenate the results of.\n",
        "        \"\"\"\n",
        "        self.H2w = nn.Parameter(winit(5*5*8, 12, 8, 5, 5))\n",
        "        self.H2b = nn.Parameter(torch.zeros(12, 4, 4)) # presumably init to zero for biases\n",
        "        assert self.H2w.nelement() + self.H2b.nelement() == 2592\n",
        "        macs += (5*5*8) * (4*4) * 12\n",
        "        acts += (4*4) * 12\n",
        "\n",
        "        # H3 is a fully connected layer\n",
        "        self.H3w = nn.Parameter(winit(4*4*12, 4*4*12, 30))\n",
        "        self.H3b = nn.Parameter(torch.zeros(30))\n",
        "        assert self.H3w.nelement() + self.H3b.nelement() == 5790\n",
        "        macs += (4*4*12) * 30\n",
        "        acts += 30\n",
        "\n",
        "        # output layer is also fully connected layer\n",
        "        self.outw = nn.Parameter(winit(30, 30, 10))\n",
        "        self.outb = nn.Parameter(-torch.ones(10)) # 9/10 targets are -1, so makes sense to init slightly towards it\n",
        "        assert self.outw.nelement() + self.outb.nelement() == 310\n",
        "        macs += 30 * 10\n",
        "        acts += 10\n",
        "\n",
        "        self.macs = macs\n",
        "        self.acts = acts\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # x has shape (1, 1, 16, 16)\n",
        "        x = F.pad(x, (2, 2, 2, 2), 'constant', -1.0) # pad by two using constant -1 for background\n",
        "        x = F.conv2d(x, self.H1w, stride=2) + self.H1b\n",
        "        x = torch.tanh(x)\n",
        "\n",
        "        # x is now shape (1, 12, 8, 8)\n",
        "        x = F.pad(x, (2, 2, 2, 2), 'constant', -1.0) # pad by two using constant -1 for background\n",
        "        slice1 = F.conv2d(x[:, 0:8], self.H2w[0:4], stride=2) # first 4 planes look at first 8 input planes\n",
        "        slice2 = F.conv2d(x[:, 4:12], self.H2w[4:8], stride=2) # next 4 planes look at last 8 input planes\n",
        "        slice3 = F.conv2d(torch.cat((x[:, 0:4], x[:, 8:12]), dim=1), self.H2w[8:12], stride=2) # last 4 planes are cross\n",
        "        x = torch.cat((slice1, slice2, slice3), dim=1) + self.H2b\n",
        "        x = torch.tanh(x)\n",
        "\n",
        "        # x is now shape (1, 12, 4, 4)\n",
        "        x = x.flatten(start_dim=1) # (1, 12*4*4)\n",
        "        x = x @ self.H3w + self.H3b\n",
        "        x = torch.tanh(x)\n",
        "\n",
        "        # x is now shape (1, 30)\n",
        "        x = x @ self.outw + self.outb\n",
        "        # Note: we deleted the tanh activation here!\n",
        "\n",
        "         # x is finally shape (1, 10)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmEeK7F-7EWa",
        "outputId": "fae4ef8e-8435-418b-b36d-8e9188b9cb77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model stats:\n",
            "# params:       9760\n",
            "# MACs:         63660\n",
            "# activations:  1000\n",
            "1\n",
            "eval: split train. loss 2.547408e-01. error 7.57%. misses: 552\n",
            "eval: split test . loss 2.883549e-01. error 8.67%. misses: 174\n",
            "2\n",
            "eval: split train. loss 1.863126e-01. error 5.76%. misses: 419\n",
            "eval: split test . loss 2.397452e-01. error 7.42%. misses: 149\n",
            "3\n",
            "eval: split train. loss 1.585573e-01. error 4.58%. misses: 333\n",
            "eval: split test . loss 2.222673e-01. error 6.88%. misses: 138\n",
            "4\n",
            "eval: split train. loss 1.367183e-01. error 4.14%. misses: 301\n",
            "eval: split test . loss 2.259786e-01. error 7.22%. misses: 144\n",
            "5\n",
            "eval: split train. loss 1.080973e-01. error 3.46%. misses: 252\n",
            "eval: split test . loss 2.042281e-01. error 6.08%. misses: 122\n",
            "6\n",
            "eval: split train. loss 9.097695e-02. error 3.07%. misses: 223\n",
            "eval: split test . loss 2.100919e-01. error 6.38%. misses: 128\n",
            "7\n",
            "eval: split train. loss 6.656891e-02. error 2.02%. misses: 146\n",
            "eval: split test . loss 1.925825e-01. error 5.78%. misses: 115\n",
            "8\n",
            "eval: split train. loss 5.637589e-02. error 1.59%. misses: 115\n",
            "eval: split test . loss 1.835809e-01. error 5.08%. misses: 102\n",
            "9\n",
            "eval: split train. loss 3.941497e-02. error 1.04%. misses: 76\n",
            "eval: split test . loss 1.715737e-01. error 4.93%. misses: 99\n",
            "10\n",
            "eval: split train. loss 6.521906e-02. error 1.99%. misses: 145\n",
            "eval: split test . loss 2.220312e-01. error 5.78%. misses: 115\n",
            "11\n",
            "eval: split train. loss 3.662320e-02. error 0.95%. misses: 69\n",
            "eval: split test . loss 1.810715e-01. error 4.93%. misses: 99\n",
            "12\n",
            "eval: split train. loss 2.958677e-02. error 0.82%. misses: 59\n",
            "eval: split test . loss 1.881249e-01. error 4.88%. misses: 97\n",
            "13\n",
            "eval: split train. loss 3.008033e-02. error 0.92%. misses: 67\n",
            "eval: split test . loss 1.884471e-01. error 4.93%. misses: 99\n",
            "14\n",
            "eval: split train. loss 4.093269e-02. error 1.41%. misses: 102\n",
            "eval: split test . loss 2.013449e-01. error 5.33%. misses: 107\n",
            "15\n",
            "eval: split train. loss 1.802801e-02. error 0.47%. misses: 33\n",
            "eval: split test . loss 1.768574e-01. error 4.88%. misses: 97\n",
            "16\n",
            "eval: split train. loss 1.130944e-02. error 0.19%. misses: 13\n",
            "eval: split test . loss 1.778838e-01. error 4.73%. misses: 94\n",
            "17\n",
            "eval: split train. loss 8.405771e-03. error 0.10%. misses: 6\n",
            "eval: split test . loss 1.747114e-01. error 4.43%. misses: 89\n",
            "18\n",
            "eval: split train. loss 6.198277e-03. error 0.04%. misses: 3\n",
            "eval: split test . loss 1.767724e-01. error 4.68%. misses: 94\n",
            "19\n",
            "eval: split train. loss 4.898855e-03. error 0.01%. misses: 0\n",
            "eval: split test . loss 1.788183e-01. error 4.58%. misses: 92\n",
            "20\n",
            "eval: split train. loss 4.077896e-03. error 0.01%. misses: 0\n",
            "eval: split test . loss 1.806057e-01. error 4.53%. misses: 91\n",
            "21\n",
            "eval: split train. loss 3.543480e-03. error 0.00%. misses: 0\n",
            "eval: split test . loss 1.821569e-01. error 4.58%. misses: 92\n",
            "22\n",
            "eval: split train. loss 3.133105e-03. error 0.00%. misses: 0\n",
            "eval: split test . loss 1.836734e-01. error 4.58%. misses: 92\n",
            "23\n",
            "eval: split train. loss 2.808620e-03. error 0.00%. misses: 0\n",
            "eval: split test . loss 1.850846e-01. error 4.58%. misses: 92\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Note: with the original learning rate, the SGD did not seem to learn well\n",
        "# so I changed it to a smaller learning rate\n",
        "learning_rate = 0.01\n",
        "\n",
        "# init rng\n",
        "torch.manual_seed(1337)\n",
        "np.random.seed(1337)\n",
        "torch.use_deterministic_algorithms(True)\n",
        "\n",
        "# init a model\n",
        "model = ModernLossNet()\n",
        "print(\"model stats:\")\n",
        "print(\"# params:      \", sum(p.numel() for p in model.parameters())) # in paper total is 9,760\n",
        "print(\"# MACs:        \", model.macs)\n",
        "print(\"# activations: \", model.acts)\n",
        "\n",
        "# init data\n",
        "Xtr, Ytr = torch.load('train1989.pt')\n",
        "Xte, Yte = torch.load('test1989.pt')\n",
        "\n",
        "# init optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "def eval_split(split):\n",
        "    # eval the full train/test set, batched implementation for efficiency\n",
        "    model.eval()\n",
        "    X, Y = (Xtr, Ytr) if split == 'train' else (Xte, Yte)\n",
        "    Yhat = model(X)\n",
        "    # Note: the updated loss function!\n",
        "    loss = F.cross_entropy(Yhat, Y.argmax(dim=1))\n",
        "    err = torch.mean((Y.argmax(dim=1) != Yhat.argmax(dim=1)).float())\n",
        "    print(f\"eval: split {split:5s}. loss {loss.item():e}. error {err.item()*100:.2f}%. misses: {int(err.item()*Y.size(0))}\")\n",
        "\n",
        "# train\n",
        "for pass_num in range(23):\n",
        "\n",
        "    # perform one epoch of training\n",
        "    model.train()\n",
        "    for step_num in range(Xtr.size(0)):\n",
        "\n",
        "        # fetch a single example into a batch of 1\n",
        "        x, y = Xtr[[step_num]], Ytr[[step_num]]\n",
        "\n",
        "        # forward the model and the loss\n",
        "        yhat = model(x)\n",
        "        loss = F.cross_entropy(yhat, y.argmax(dim=1))\n",
        "        # Note: the updated loss function!\n",
        "\n",
        "        # calculate the gradient and update the parameters\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # after epoch epoch evaluate the train and test error / metrics\n",
        "    print(pass_num + 1)\n",
        "    eval_split('train')\n",
        "    eval_split('test')\n",
        "\n",
        "# save final model to file\n",
        "torch.save(model.state_dict(), 'crossentropy_model.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40vGd5mxh4TV"
      },
      "source": [
        "The test performance is similar, maybe a little worse - since I reduced the learning rate, I may need to increase the number of passes to compensate.\n",
        "\n",
        "But, we are now seeing zero error on the training set. We may be overfitting.\n",
        "\n",
        "Fortunately, we have some \"tricks\" for improving the performance of deep neural networks! We can try - \n",
        "\n",
        "* a more modern optimizer (i.e. improve over the stochastic gradient descent with fixed learning rate)\n",
        "* data augmentation\n",
        "* a regularization technique, like dropout\n",
        "\n",
        "and see if they improve our performance, while still keeping the basic model - number of layers, number of units, size of each convolutional filter - the same."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
