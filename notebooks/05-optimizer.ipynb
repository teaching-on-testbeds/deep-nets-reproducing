{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ifLOPkjJsfy"
      },
      "source": [
        "### Training with a better optimizer"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QCA0umE7jJBL"
      },
      "source": [
        "The original model is trained using stochastic gradient descent on a single sample at a time, with a fixed learning rate.\n",
        "\n",
        "To see if a more sophisticated optimizer will help, we will try to train the same model as in the previous notebook, but with a version of the Adam optimizer. Adam is a combination of two gradient descent \"variations\" we have discussed previously:\n",
        "\n",
        "* Momentum - uses a velocity vector that accumulates gradient of past steps.\n",
        "* RMSProp - normalizes parameter update using per-parameter EWMA of square of gradient.\n",
        "\n",
        "and it is often used as a starting point when training deep neural networks. \n",
        "\n",
        "We are also going to \"decay\" the learning rate as we iterate over the number of epochs.\n",
        "\n",
        "And, we will also double the number of training epochs from 23 to 46."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJqRxqAdelLg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "torch.set_num_threads(2) # for performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SlWnqk_t6bEX"
      },
      "outputs": [],
      "source": [
        "\n",
        "class ModernLossNet(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # initialization as described in the paper to my best ability, but it doesn't look right...\n",
        "        winit = lambda fan_in, *shape: (torch.rand(*shape) - 0.5) * 2 * 2.4 / fan_in**0.5\n",
        "        macs = 0 # keep track of MACs (multiply accumulates)\n",
        "        acts = 0 # keep track of number of activations\n",
        "\n",
        "        # H1 layer parameters and their initialization\n",
        "        self.H1w = nn.Parameter(winit(5*5*1, 12, 1, 5, 5))\n",
        "        self.H1b = nn.Parameter(torch.zeros(12, 8, 8)) # presumably init to zero for biases\n",
        "        assert self.H1w.nelement() + self.H1b.nelement() == 1068\n",
        "        macs += (5*5*1) * (8*8) * 12\n",
        "        acts += (8*8) * 12\n",
        "\n",
        "        # H2 layer parameters and their initialization\n",
        "        \"\"\"\n",
        "        H2 neurons all connect to only 8 of the 12 input planes, with an unspecified pattern\n",
        "        I am going to assume the most sensible block pattern where 4 planes at a time connect\n",
        "        to differently overlapping groups of 8/12 input planes. We will implement this with 3\n",
        "        separate convolutions that we concatenate the results of.\n",
        "        \"\"\"\n",
        "        self.H2w = nn.Parameter(winit(5*5*8, 12, 8, 5, 5))\n",
        "        self.H2b = nn.Parameter(torch.zeros(12, 4, 4)) # presumably init to zero for biases\n",
        "        assert self.H2w.nelement() + self.H2b.nelement() == 2592\n",
        "        macs += (5*5*8) * (4*4) * 12\n",
        "        acts += (4*4) * 12\n",
        "\n",
        "        # H3 is a fully connected layer\n",
        "        self.H3w = nn.Parameter(winit(4*4*12, 4*4*12, 30))\n",
        "        self.H3b = nn.Parameter(torch.zeros(30))\n",
        "        assert self.H3w.nelement() + self.H3b.nelement() == 5790\n",
        "        macs += (4*4*12) * 30\n",
        "        acts += 30\n",
        "\n",
        "        # output layer is also fully connected layer\n",
        "        self.outw = nn.Parameter(winit(30, 30, 10))\n",
        "        self.outb = nn.Parameter(-torch.ones(10)) # 9/10 targets are -1, so makes sense to init slightly towards it\n",
        "        assert self.outw.nelement() + self.outb.nelement() == 310\n",
        "        macs += 30 * 10\n",
        "        acts += 10\n",
        "\n",
        "        self.macs = macs\n",
        "        self.acts = acts\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # x has shape (1, 1, 16, 16)\n",
        "        x = F.pad(x, (2, 2, 2, 2), 'constant', -1.0) # pad by two using constant -1 for background\n",
        "        x = F.conv2d(x, self.H1w, stride=2) + self.H1b\n",
        "        x = torch.tanh(x)\n",
        "\n",
        "        # x is now shape (1, 12, 8, 8)\n",
        "        x = F.pad(x, (2, 2, 2, 2), 'constant', -1.0) # pad by two using constant -1 for background\n",
        "        slice1 = F.conv2d(x[:, 0:8], self.H2w[0:4], stride=2) # first 4 planes look at first 8 input planes\n",
        "        slice2 = F.conv2d(x[:, 4:12], self.H2w[4:8], stride=2) # next 4 planes look at last 8 input planes\n",
        "        slice3 = F.conv2d(torch.cat((x[:, 0:4], x[:, 8:12]), dim=1), self.H2w[8:12], stride=2) # last 4 planes are cross\n",
        "        x = torch.cat((slice1, slice2, slice3), dim=1) + self.H2b\n",
        "        x = torch.tanh(x)\n",
        "\n",
        "        # x is now shape (1, 12, 4, 4)\n",
        "        x = x.flatten(start_dim=1) # (1, 12*4*4)\n",
        "        x = x @ self.H3w + self.H3b\n",
        "        x = torch.tanh(x)\n",
        "\n",
        "        # x is now shape (1, 30)\n",
        "        x = x @ self.outw + self.outb\n",
        "        # Note: we deleted the tanh activation here!\n",
        "\n",
        "         # x is finally shape (1, 10)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VD7pKr5dJ8FJ",
        "outputId": "1bea4eb5-f31d-4ac1-804e-c7be11a4d0f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model stats:\n",
            "# params:       9760\n",
            "# MACs:         63660\n",
            "# activations:  1000\n",
            "1\n",
            "eval: split train. loss 3.553850e-01. error 10.08%. misses: 734\n",
            "eval: split test . loss 3.418252e-01. error 10.31%. misses: 206\n",
            "2\n",
            "eval: split train. loss 2.473212e-01. error 7.20%. misses: 524\n",
            "eval: split test . loss 2.514379e-01. error 7.22%. misses: 144\n",
            "3\n",
            "eval: split train. loss 1.973256e-01. error 5.58%. misses: 406\n",
            "eval: split test . loss 2.185514e-01. error 6.13%. misses: 122\n",
            "4\n",
            "eval: split train. loss 1.690831e-01. error 4.77%. misses: 347\n",
            "eval: split test . loss 2.048475e-01. error 5.73%. misses: 115\n",
            "5\n",
            "eval: split train. loss 1.465219e-01. error 4.32%. misses: 314\n",
            "eval: split test . loss 1.939704e-01. error 5.28%. misses: 105\n",
            "6\n",
            "eval: split train. loss 1.272241e-01. error 3.72%. misses: 271\n",
            "eval: split test . loss 1.844373e-01. error 5.18%. misses: 104\n",
            "7\n",
            "eval: split train. loss 1.110661e-01. error 3.13%. misses: 228\n",
            "eval: split test . loss 1.769724e-01. error 4.98%. misses: 99\n",
            "8\n",
            "eval: split train. loss 9.672461e-02. error 2.81%. misses: 204\n",
            "eval: split test . loss 1.703543e-01. error 4.88%. misses: 97\n",
            "9\n",
            "eval: split train. loss 8.364436e-02. error 2.51%. misses: 183\n",
            "eval: split test . loss 1.640323e-01. error 4.38%. misses: 87\n",
            "10\n",
            "eval: split train. loss 7.263264e-02. error 2.14%. misses: 156\n",
            "eval: split test . loss 1.587063e-01. error 4.19%. misses: 84\n",
            "11\n",
            "eval: split train. loss 6.405640e-02. error 2.00%. misses: 145\n",
            "eval: split test . loss 1.555053e-01. error 4.14%. misses: 82\n",
            "12\n",
            "eval: split train. loss 5.734518e-02. error 1.74%. misses: 126\n",
            "eval: split test . loss 1.540471e-01. error 4.29%. misses: 86\n",
            "13\n",
            "eval: split train. loss 5.191027e-02. error 1.56%. misses: 114\n",
            "eval: split test . loss 1.537017e-01. error 4.24%. misses: 84\n",
            "14\n",
            "eval: split train. loss 4.761230e-02. error 1.37%. misses: 100\n",
            "eval: split test . loss 1.541632e-01. error 4.19%. misses: 84\n",
            "15\n",
            "eval: split train. loss 4.361586e-02. error 1.25%. misses: 90\n",
            "eval: split test . loss 1.546300e-01. error 4.09%. misses: 82\n",
            "16\n",
            "eval: split train. loss 3.840218e-02. error 1.04%. misses: 76\n",
            "eval: split test . loss 1.535889e-01. error 3.94%. misses: 79\n",
            "17\n",
            "eval: split train. loss 3.259112e-02. error 0.93%. misses: 67\n",
            "eval: split test . loss 1.515513e-01. error 3.94%. misses: 79\n",
            "18\n",
            "eval: split train. loss 2.806237e-02. error 0.81%. misses: 59\n",
            "eval: split test . loss 1.497022e-01. error 3.79%. misses: 76\n",
            "19\n",
            "eval: split train. loss 2.487626e-02. error 0.59%. misses: 43\n",
            "eval: split test . loss 1.488707e-01. error 3.74%. misses: 74\n",
            "20\n",
            "eval: split train. loss 2.168269e-02. error 0.52%. misses: 38\n",
            "eval: split test . loss 1.478305e-01. error 3.79%. misses: 76\n",
            "21\n",
            "eval: split train. loss 1.854182e-02. error 0.41%. misses: 29\n",
            "eval: split test . loss 1.475250e-01. error 3.84%. misses: 77\n",
            "22\n",
            "eval: split train. loss 1.562811e-02. error 0.34%. misses: 25\n",
            "eval: split test . loss 1.469952e-01. error 3.79%. misses: 76\n",
            "23\n",
            "eval: split train. loss 1.478657e-02. error 0.38%. misses: 27\n",
            "eval: split test . loss 1.517997e-01. error 3.99%. misses: 79\n",
            "24\n",
            "eval: split train. loss 1.270418e-02. error 0.34%. misses: 25\n",
            "eval: split test . loss 1.526654e-01. error 3.84%. misses: 77\n",
            "25\n",
            "eval: split train. loss 1.228933e-02. error 0.26%. misses: 19\n",
            "eval: split test . loss 1.510760e-01. error 4.04%. misses: 81\n",
            "26\n",
            "eval: split train. loss 1.102349e-02. error 0.26%. misses: 19\n",
            "eval: split test . loss 1.577505e-01. error 4.19%. misses: 84\n",
            "27\n",
            "eval: split train. loss 8.824149e-03. error 0.21%. misses: 14\n",
            "eval: split test . loss 1.540242e-01. error 3.84%. misses: 77\n",
            "28\n",
            "eval: split train. loss 8.945066e-03. error 0.15%. misses: 11\n",
            "eval: split test . loss 1.569294e-01. error 3.79%. misses: 76\n",
            "29\n",
            "eval: split train. loss 7.410577e-03. error 0.12%. misses: 8\n",
            "eval: split test . loss 1.583304e-01. error 4.19%. misses: 84\n",
            "30\n",
            "eval: split train. loss 7.714382e-03. error 0.16%. misses: 12\n",
            "eval: split test . loss 1.619555e-01. error 4.19%. misses: 84\n",
            "31\n",
            "eval: split train. loss 7.485712e-03. error 0.21%. misses: 14\n",
            "eval: split test . loss 1.610946e-01. error 4.19%. misses: 84\n",
            "32\n",
            "eval: split train. loss 4.751204e-03. error 0.08%. misses: 6\n",
            "eval: split test . loss 1.587567e-01. error 3.99%. misses: 79\n",
            "33\n",
            "eval: split train. loss 5.644011e-03. error 0.12%. misses: 8\n",
            "eval: split test . loss 1.617115e-01. error 4.04%. misses: 81\n",
            "34\n",
            "eval: split train. loss 4.707410e-03. error 0.08%. misses: 6\n",
            "eval: split test . loss 1.624220e-01. error 4.09%. misses: 82\n",
            "35\n",
            "eval: split train. loss 3.832362e-03. error 0.07%. misses: 5\n",
            "eval: split test . loss 1.602481e-01. error 3.94%. misses: 79\n",
            "36\n",
            "eval: split train. loss 3.568698e-03. error 0.08%. misses: 6\n",
            "eval: split test . loss 1.615783e-01. error 4.09%. misses: 82\n",
            "37\n",
            "eval: split train. loss 3.764395e-03. error 0.08%. misses: 6\n",
            "eval: split test . loss 1.664671e-01. error 4.09%. misses: 82\n",
            "38\n",
            "eval: split train. loss 2.188502e-03. error 0.05%. misses: 3\n",
            "eval: split test . loss 1.645725e-01. error 3.64%. misses: 72\n",
            "39\n",
            "eval: split train. loss 2.375670e-03. error 0.03%. misses: 1\n",
            "eval: split test . loss 1.699141e-01. error 3.94%. misses: 79\n",
            "40\n",
            "eval: split train. loss 1.841924e-03. error 0.01%. misses: 0\n",
            "eval: split test . loss 1.748033e-01. error 3.89%. misses: 77\n",
            "41\n",
            "eval: split train. loss 2.638372e-03. error 0.03%. misses: 1\n",
            "eval: split test . loss 1.705838e-01. error 4.04%. misses: 81\n",
            "42\n",
            "eval: split train. loss 2.395150e-03. error 0.03%. misses: 1\n",
            "eval: split test . loss 1.711865e-01. error 4.09%. misses: 82\n",
            "43\n",
            "eval: split train. loss 1.728846e-03. error 0.03%. misses: 1\n",
            "eval: split test . loss 1.713103e-01. error 3.99%. misses: 79\n",
            "44\n",
            "eval: split train. loss 1.570983e-03. error 0.03%. misses: 1\n",
            "eval: split test . loss 1.719362e-01. error 3.94%. misses: 79\n",
            "45\n",
            "eval: split train. loss 1.439892e-03. error 0.01%. misses: 0\n",
            "eval: split test . loss 1.732105e-01. error 3.94%. misses: 79\n",
            "46\n",
            "eval: split train. loss 1.193036e-03. error 0.01%. misses: 0\n",
            "eval: split test . loss 1.744287e-01. error 3.89%. misses: 77\n"
          ]
        }
      ],
      "source": [
        "# The following code is adapted from \"modern.py\" in the repository.\n",
        "\n",
        "# Note: different initial learning rate\n",
        "learning_rate = 3e-4\n",
        "\n",
        "# init rng\n",
        "torch.manual_seed(1337)\n",
        "np.random.seed(1337)\n",
        "torch.use_deterministic_algorithms(True)\n",
        "\n",
        "# init a model\n",
        "model = ModernLossNet()\n",
        "print(\"model stats:\")\n",
        "print(\"# params:      \", sum(p.numel() for p in model.parameters())) # in paper total is 9,760\n",
        "print(\"# MACs:        \", model.macs)\n",
        "print(\"# activations: \", model.acts)\n",
        "\n",
        "# init data\n",
        "Xtr, Ytr = torch.load('train1989.pt')\n",
        "Xte, Yte = torch.load('test1989.pt')\n",
        "\n",
        "# init optimizer\n",
        "# Note: use the the AdamW optimizer\n",
        "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "def eval_split(split):\n",
        "    # eval the full train/test set, batched implementation for efficiency\n",
        "    model.eval()\n",
        "    X, Y = (Xtr, Ytr) if split == 'train' else (Xte, Yte)\n",
        "    Yhat = model(X)\n",
        "    loss = F.cross_entropy(Yhat, Y.argmax(dim=1))\n",
        "    err = torch.mean((Y.argmax(dim=1) != Yhat.argmax(dim=1)).float())\n",
        "    print(f\"eval: split {split:5s}. loss {loss.item():e}. error {err.item()*100:.2f}%. misses: {int(err.item()*Y.size(0))}\")\n",
        "  \n",
        "# Note: increase number of training passes\n",
        "for pass_num in range(46):\n",
        "\n",
        "    # Note: also implement learning rate decay\n",
        "    alpha = pass_num / 45\n",
        "    for g in optimizer.param_groups:\n",
        "        g['lr'] = (1 - alpha) * learning_rate + alpha * (learning_rate / 3)\n",
        "\n",
        "    # perform one epoch of training\n",
        "    model.train()\n",
        "    for step_num in range(Xtr.size(0)):\n",
        "\n",
        "        # fetch a single example into a batch of 1\n",
        "        x, y = Xtr[[step_num]], Ytr[[step_num]]\n",
        "\n",
        "        # forward the model and the loss\n",
        "        yhat = model(x)\n",
        "        loss = F.cross_entropy(yhat, y.argmax(dim=1))\n",
        "\n",
        "        # calculate the gradient and update the parameters\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # after epoch epoch evaluate the train and test error / metrics\n",
        "    print(pass_num + 1)\n",
        "    eval_split('train')\n",
        "    eval_split('test')\n",
        "\n",
        "# save final model to file\n",
        "torch.save(model.state_dict(), 'adam_model.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwZPJEhqkmE_"
      },
      "source": [
        "We seem to have improved the test performance - it is now below 4% error. We are still achieving 0 error on the training set, so we remain concerned about potential overfitting."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
