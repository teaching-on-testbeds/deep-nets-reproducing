{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNi6ad2MGXcj"
      },
      "source": [
        "### Original model with larger data set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRnqXdM2nEkw"
      },
      "source": [
        "We have used a series of modern \"tricks\" to improve the performance of the 1989 model, without changing its basic structure.\n",
        "\n",
        "But one of our best \"tricks\" for improving the performance of large neural networks is to increase the size and/or quality of the data on which the model is trained.\n",
        "\n",
        "It is interesting to see what performance we could have achieved in 1989 without any of those other \"tricks\" (some of which were not yet understood or widely used) - just using more data."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TzGPovo9HNmn"
      },
      "source": [
        "\n",
        "In the following cells, we preprocess the full MNIST dataset - 50,000 training samples and 10,000 test samples, rather than the 7291 and 2007 samples used in 1989 - to downsample them from 28x28 to 16x16.\n",
        "\n",
        "Then, we'll train that original 1989 model - *without* any of the changes we tried earlier - on this larger data set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJqRxqAdelLg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchvision import datasets\n",
        "\n",
        "torch.set_num_threads(2) # for performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVPepLMnGh9x",
        "outputId": "676fe53d-ef44-4942-b067-1666a5548c69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 71638042.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 9457814.95it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 39912114.62it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 9734557.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# The following code is adapted from \"prepro.py\" in the repository.\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "np.random.seed(1337)\n",
        "\n",
        "for split in ['test', 'train']:\n",
        "\n",
        "    data = datasets.MNIST('./data', train=split=='train', download=True)\n",
        "\n",
        "    # Note: use full size data set\n",
        "    n = 50000 if split == 'train' else 10000\n",
        "    rp = np.random.permutation(len(data))[:n]\n",
        "\n",
        "    X = torch.full((n, 1, 16, 16), 0.0, dtype=torch.float32)\n",
        "    Y = torch.full((n, 10), -1.0, dtype=torch.float32)\n",
        "    for i, ix in enumerate(rp):\n",
        "        I, yint = data[int(ix)]\n",
        "        # PIL image -> numpy -> torch tensor -> [-1, 1] fp32\n",
        "        xi = torch.from_numpy(np.array(I, dtype=np.float32)) / 127.5 - 1.0\n",
        "        # add a fake batch dimension and a channel dimension of 1 or F.interpolate won't be happy\n",
        "        xi = xi[None, None, ...]\n",
        "        # resize to (16, 16) images with bilinear interpolation\n",
        "        xi = F.interpolate(xi, (16, 16), mode='bilinear')\n",
        "        X[i] = xi[0] # store\n",
        "\n",
        "        # set the correct class to have target of +1.0\n",
        "        Y[i, yint] = 1.0\n",
        "\n",
        "    torch.save((X, Y), split + 'full.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The following code is adapted from \"repro.py\" in the repository.\n",
        "\n",
        "class BaseNet(nn.Module):\n",
        "    \"\"\" 1989 LeCun ConvNet per description in the paper \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # initialization as described in the paper to my best ability, but it doesn't look right...\n",
        "        winit = lambda fan_in, *shape: (torch.rand(*shape) - 0.5) * 2 * 2.4 / fan_in**0.5\n",
        "        macs = 0 # keep track of MACs (multiply accumulates)\n",
        "        acts = 0 # keep track of number of activations\n",
        "\n",
        "        # H1 layer parameters and their initialization\n",
        "        self.H1w = nn.Parameter(winit(5*5*1, 12, 1, 5, 5))\n",
        "        self.H1b = nn.Parameter(torch.zeros(12, 8, 8)) # presumably init to zero for biases\n",
        "        assert self.H1w.nelement() + self.H1b.nelement() == 1068\n",
        "        macs += (5*5*1) * (8*8) * 12\n",
        "        acts += (8*8) * 12\n",
        "\n",
        "        # H2 layer parameters and their initialization\n",
        "        \"\"\"\n",
        "        H2 neurons all connect to only 8 of the 12 input planes, with an unspecified pattern\n",
        "        I am going to assume the most sensible block pattern where 4 planes at a time connect\n",
        "        to differently overlapping groups of 8/12 input planes. We will implement this with 3\n",
        "        separate convolutions that we concatenate the results of.\n",
        "        \"\"\"\n",
        "        self.H2w = nn.Parameter(winit(5*5*8, 12, 8, 5, 5))\n",
        "        self.H2b = nn.Parameter(torch.zeros(12, 4, 4)) # presumably init to zero for biases\n",
        "        assert self.H2w.nelement() + self.H2b.nelement() == 2592\n",
        "        macs += (5*5*8) * (4*4) * 12\n",
        "        acts += (4*4) * 12\n",
        "\n",
        "        # H3 is a fully connected layer\n",
        "        self.H3w = nn.Parameter(winit(4*4*12, 4*4*12, 30))\n",
        "        self.H3b = nn.Parameter(torch.zeros(30))\n",
        "        assert self.H3w.nelement() + self.H3b.nelement() == 5790\n",
        "        macs += (4*4*12) * 30\n",
        "        acts += 30\n",
        "\n",
        "        # output layer is also fully connected layer\n",
        "        self.outw = nn.Parameter(winit(30, 30, 10))\n",
        "        self.outb = nn.Parameter(-torch.ones(10)) # 9/10 targets are -1, so makes sense to init slightly towards it\n",
        "        assert self.outw.nelement() + self.outb.nelement() == 310\n",
        "        macs += 30 * 10\n",
        "        acts += 10\n",
        "\n",
        "        self.macs = macs\n",
        "        self.acts = acts\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # x has shape (1, 1, 16, 16)\n",
        "        x = F.pad(x, (2, 2, 2, 2), 'constant', -1.0) # pad by two using constant -1 for background\n",
        "        x = F.conv2d(x, self.H1w, stride=2) + self.H1b\n",
        "        x = torch.tanh(x)\n",
        "\n",
        "        # x is now shape (1, 12, 8, 8)\n",
        "        x = F.pad(x, (2, 2, 2, 2), 'constant', -1.0) # pad by two using constant -1 for background\n",
        "        slice1 = F.conv2d(x[:, 0:8], self.H2w[0:4], stride=2) # first 4 planes look at first 8 input planes\n",
        "        slice2 = F.conv2d(x[:, 4:12], self.H2w[4:8], stride=2) # next 4 planes look at last 8 input planes\n",
        "        slice3 = F.conv2d(torch.cat((x[:, 0:4], x[:, 8:12]), dim=1), self.H2w[8:12], stride=2) # last 4 planes are cross\n",
        "        x = torch.cat((slice1, slice2, slice3), dim=1) + self.H2b\n",
        "        x = torch.tanh(x)\n",
        "\n",
        "        # x is now shape (1, 12, 4, 4)\n",
        "        x = x.flatten(start_dim=1) # (1, 12*4*4)\n",
        "        x = x @ self.H3w + self.H3b\n",
        "        x = torch.tanh(x)\n",
        "\n",
        "        # x is now shape (1, 30)\n",
        "        x = x @ self.outw + self.outb\n",
        "        x = torch.tanh(x)\n",
        "\n",
        "         # x is finally shape (1, 10)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhDIqjF-Ge09",
        "outputId": "c9ec7e84-13d8-45b7-bdae-cb1c1fc3333a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model stats:\n",
            "# params:       9760\n",
            "# MACs:         63660\n",
            "# activations:  1000\n",
            "1\n",
            "eval: split train. loss 3.372094e-02. error 5.11%. misses: 2555\n",
            "eval: split test . loss 3.295155e-02. error 5.04%. misses: 504\n",
            "2\n",
            "eval: split train. loss 2.436154e-02. error 3.58%. misses: 1790\n",
            "eval: split test . loss 2.541548e-02. error 3.73%. misses: 373\n",
            "3\n",
            "eval: split train. loss 2.174933e-02. error 3.24%. misses: 1620\n",
            "eval: split test . loss 2.369628e-02. error 3.57%. misses: 357\n",
            "4\n",
            "eval: split train. loss 2.092772e-02. error 3.16%. misses: 1579\n",
            "eval: split test . loss 2.357459e-02. error 3.49%. misses: 348\n",
            "5\n",
            "eval: split train. loss 1.849931e-02. error 2.74%. misses: 1369\n",
            "eval: split test . loss 2.216178e-02. error 3.43%. misses: 342\n",
            "6\n",
            "eval: split train. loss 1.842882e-02. error 2.75%. misses: 1372\n",
            "eval: split test . loss 2.217882e-02. error 3.43%. misses: 342\n",
            "7\n",
            "eval: split train. loss 1.781441e-02. error 2.66%. misses: 1329\n",
            "eval: split test . loss 2.282108e-02. error 3.53%. misses: 353\n",
            "8\n",
            "eval: split train. loss 1.688741e-02. error 2.44%. misses: 1219\n",
            "eval: split test . loss 2.211263e-02. error 3.30%. misses: 329\n",
            "9\n",
            "eval: split train. loss 1.672671e-02. error 2.48%. misses: 1241\n",
            "eval: split test . loss 2.154214e-02. error 3.26%. misses: 326\n",
            "10\n",
            "eval: split train. loss 1.639496e-02. error 2.43%. misses: 1216\n",
            "eval: split test . loss 2.104777e-02. error 3.17%. misses: 317\n",
            "11\n",
            "eval: split train. loss 1.579655e-02. error 2.29%. misses: 1142\n",
            "eval: split test . loss 2.116732e-02. error 3.34%. misses: 333\n",
            "12\n",
            "eval: split train. loss 1.650465e-02. error 2.41%. misses: 1202\n",
            "eval: split test . loss 2.115814e-02. error 3.20%. misses: 320\n",
            "13\n",
            "eval: split train. loss 1.721802e-02. error 2.57%. misses: 1286\n",
            "eval: split test . loss 2.268604e-02. error 3.45%. misses: 344\n",
            "14\n",
            "eval: split train. loss 1.601706e-02. error 2.39%. misses: 1193\n",
            "eval: split test . loss 2.209910e-02. error 3.31%. misses: 331\n",
            "15\n",
            "eval: split train. loss 1.883638e-02. error 2.76%. misses: 1379\n",
            "eval: split test . loss 2.387411e-02. error 3.59%. misses: 359\n",
            "16\n",
            "eval: split train. loss 1.424371e-02. error 2.06%. misses: 1032\n",
            "eval: split test . loss 1.960425e-02. error 3.00%. misses: 299\n",
            "17\n",
            "eval: split train. loss 1.585988e-02. error 2.31%. misses: 1157\n",
            "eval: split test . loss 2.068493e-02. error 3.12%. misses: 311\n",
            "18\n",
            "eval: split train. loss 1.498585e-02. error 2.13%. misses: 1064\n",
            "eval: split test . loss 2.080031e-02. error 3.03%. misses: 303\n",
            "19\n",
            "eval: split train. loss 1.489681e-02. error 2.15%. misses: 1073\n",
            "eval: split test . loss 2.115317e-02. error 3.23%. misses: 322\n",
            "20\n",
            "eval: split train. loss 1.410568e-02. error 1.99%. misses: 992\n",
            "eval: split test . loss 2.046336e-02. error 3.13%. misses: 313\n",
            "21\n",
            "eval: split train. loss 1.445430e-02. error 2.09%. misses: 1045\n",
            "eval: split test . loss 1.987544e-02. error 2.86%. misses: 285\n",
            "22\n",
            "eval: split train. loss 1.388672e-02. error 1.99%. misses: 997\n",
            "eval: split test . loss 2.034056e-02. error 3.01%. misses: 300\n",
            "23\n",
            "eval: split train. loss 1.450572e-02. error 2.07%. misses: 1037\n",
            "eval: split test . loss 2.144059e-02. error 3.34%. misses: 333\n",
            "24\n",
            "eval: split train. loss 1.498120e-02. error 2.15%. misses: 1073\n",
            "eval: split test . loss 2.252585e-02. error 3.43%. misses: 342\n",
            "25\n",
            "eval: split train. loss 1.421501e-02. error 2.12%. misses: 1059\n",
            "eval: split test . loss 2.035109e-02. error 3.09%. misses: 308\n",
            "26\n",
            "eval: split train. loss 1.254898e-02. error 1.74%. misses: 870\n",
            "eval: split test . loss 2.123045e-02. error 3.18%. misses: 318\n",
            "27\n",
            "eval: split train. loss 1.614271e-02. error 2.38%. misses: 1188\n",
            "eval: split test . loss 2.271791e-02. error 3.28%. misses: 328\n",
            "28\n",
            "eval: split train. loss 1.384461e-02. error 2.00%. misses: 997\n",
            "eval: split test . loss 2.152014e-02. error 3.26%. misses: 326\n",
            "29\n",
            "eval: split train. loss 1.325278e-02. error 1.89%. misses: 942\n",
            "eval: split test . loss 2.086317e-02. error 3.07%. misses: 307\n",
            "30\n",
            "eval: split train. loss 1.196145e-02. error 1.68%. misses: 841\n",
            "eval: split test . loss 2.074786e-02. error 3.11%. misses: 310\n",
            "31\n",
            "eval: split train. loss 1.159985e-02. error 1.67%. misses: 832\n",
            "eval: split test . loss 1.989081e-02. error 3.06%. misses: 306\n",
            "32\n",
            "eval: split train. loss 1.268112e-02. error 1.81%. misses: 903\n",
            "eval: split test . loss 2.024861e-02. error 2.99%. misses: 298\n",
            "33\n",
            "eval: split train. loss 1.374121e-02. error 2.01%. misses: 1006\n",
            "eval: split test . loss 2.151244e-02. error 3.22%. misses: 322\n",
            "34\n",
            "eval: split train. loss 1.224877e-02. error 1.76%. misses: 877\n",
            "eval: split test . loss 2.057443e-02. error 2.99%. misses: 298\n",
            "35\n",
            "eval: split train. loss 1.560556e-02. error 2.23%. misses: 1113\n",
            "eval: split test . loss 2.459020e-02. error 3.55%. misses: 355\n",
            "36\n",
            "eval: split train. loss 1.278026e-02. error 1.83%. misses: 915\n",
            "eval: split test . loss 2.126031e-02. error 3.13%. misses: 313\n",
            "37\n",
            "eval: split train. loss 1.442042e-02. error 2.00%. misses: 999\n",
            "eval: split test . loss 2.277985e-02. error 3.34%. misses: 333\n",
            "38\n",
            "eval: split train. loss 1.483064e-02. error 2.09%. misses: 1045\n",
            "eval: split test . loss 2.252653e-02. error 3.32%. misses: 331\n",
            "39\n",
            "eval: split train. loss 1.394624e-02. error 1.97%. misses: 987\n",
            "eval: split test . loss 2.171374e-02. error 3.14%. misses: 313\n",
            "40\n",
            "eval: split train. loss 1.324141e-02. error 1.89%. misses: 946\n",
            "eval: split test . loss 2.128308e-02. error 3.16%. misses: 315\n",
            "41\n",
            "eval: split train. loss 1.220651e-02. error 1.72%. misses: 860\n",
            "eval: split test . loss 1.986328e-02. error 2.93%. misses: 293\n",
            "42\n",
            "eval: split train. loss 1.391113e-02. error 2.00%. misses: 999\n",
            "eval: split test . loss 2.288177e-02. error 3.38%. misses: 337\n",
            "43\n",
            "eval: split train. loss 1.405868e-02. error 2.07%. misses: 1032\n",
            "eval: split test . loss 2.133248e-02. error 3.26%. misses: 326\n",
            "44\n",
            "eval: split train. loss 1.225914e-02. error 1.71%. misses: 855\n",
            "eval: split test . loss 2.044796e-02. error 3.09%. misses: 308\n",
            "45\n",
            "eval: split train. loss 1.220175e-02. error 1.70%. misses: 850\n",
            "eval: split test . loss 2.143597e-02. error 3.17%. misses: 317\n",
            "46\n",
            "eval: split train. loss 1.211486e-02. error 1.74%. misses: 872\n",
            "eval: split test . loss 2.030264e-02. error 3.04%. misses: 304\n",
            "47\n",
            "eval: split train. loss 1.220082e-02. error 1.76%. misses: 879\n",
            "eval: split test . loss 2.023935e-02. error 2.97%. misses: 296\n",
            "48\n",
            "eval: split train. loss 1.213229e-02. error 1.73%. misses: 867\n",
            "eval: split test . loss 2.152924e-02. error 3.30%. misses: 329\n",
            "49\n",
            "eval: split train. loss 1.152498e-02. error 1.63%. misses: 815\n",
            "eval: split test . loss 2.131634e-02. error 3.19%. misses: 318\n",
            "50\n",
            "eval: split train. loss 1.316070e-02. error 1.92%. misses: 958\n",
            "eval: split test . loss 2.041013e-02. error 3.04%. misses: 304\n",
            "51\n",
            "eval: split train. loss 1.286846e-02. error 1.85%. misses: 927\n",
            "eval: split test . loss 2.057673e-02. error 3.07%. misses: 307\n",
            "52\n",
            "eval: split train. loss 1.377269e-02. error 1.97%. misses: 985\n",
            "eval: split test . loss 2.184906e-02. error 3.10%. misses: 309\n",
            "53\n",
            "eval: split train. loss 1.203299e-02. error 1.72%. misses: 860\n",
            "eval: split test . loss 2.165468e-02. error 3.25%. misses: 324\n",
            "54\n",
            "eval: split train. loss 1.304484e-02. error 1.90%. misses: 949\n",
            "eval: split test . loss 2.215464e-02. error 3.39%. misses: 339\n",
            "55\n",
            "eval: split train. loss 1.378473e-02. error 1.97%. misses: 982\n",
            "eval: split test . loss 2.186319e-02. error 3.26%. misses: 326\n",
            "56\n",
            "eval: split train. loss 1.310787e-02. error 1.87%. misses: 932\n",
            "eval: split test . loss 2.128368e-02. error 3.24%. misses: 324\n",
            "57\n",
            "eval: split train. loss 1.384380e-02. error 1.91%. misses: 956\n",
            "eval: split test . loss 2.215875e-02. error 3.31%. misses: 331\n",
            "58\n",
            "eval: split train. loss 1.583852e-02. error 2.29%. misses: 1142\n",
            "eval: split test . loss 2.365907e-02. error 3.61%. misses: 361\n",
            "59\n",
            "eval: split train. loss 1.390418e-02. error 1.97%. misses: 985\n",
            "eval: split test . loss 2.218143e-02. error 3.28%. misses: 328\n",
            "60\n",
            "eval: split train. loss 1.510811e-02. error 2.17%. misses: 1085\n",
            "eval: split test . loss 2.193770e-02. error 3.10%. misses: 309\n",
            "61\n",
            "eval: split train. loss 1.127394e-02. error 1.57%. misses: 786\n",
            "eval: split test . loss 1.956165e-02. error 2.77%. misses: 276\n",
            "62\n",
            "eval: split train. loss 1.676752e-02. error 2.39%. misses: 1195\n",
            "eval: split test . loss 2.372211e-02. error 3.47%. misses: 346\n",
            "63\n",
            "eval: split train. loss 1.296359e-02. error 1.86%. misses: 929\n",
            "eval: split test . loss 2.049733e-02. error 3.12%. misses: 311\n",
            "64\n",
            "eval: split train. loss 1.141657e-02. error 1.60%. misses: 798\n",
            "eval: split test . loss 2.064254e-02. error 3.14%. misses: 313\n",
            "65\n",
            "eval: split train. loss 1.156450e-02. error 1.61%. misses: 805\n",
            "eval: split test . loss 2.024936e-02. error 2.98%. misses: 297\n",
            "66\n",
            "eval: split train. loss 1.134746e-02. error 1.57%. misses: 786\n",
            "eval: split test . loss 2.180077e-02. error 3.22%. misses: 322\n",
            "67\n",
            "eval: split train. loss 1.130116e-02. error 1.59%. misses: 796\n",
            "eval: split test . loss 2.072511e-02. error 2.93%. misses: 293\n",
            "68\n",
            "eval: split train. loss 1.341841e-02. error 1.97%. misses: 985\n",
            "eval: split test . loss 2.116446e-02. error 3.15%. misses: 315\n",
            "69\n",
            "eval: split train. loss 1.158101e-02. error 1.65%. misses: 824\n",
            "eval: split test . loss 1.993853e-02. error 2.99%. misses: 298\n",
            "70\n",
            "eval: split train. loss 1.168176e-02. error 1.64%. misses: 822\n",
            "eval: split test . loss 2.076118e-02. error 3.13%. misses: 313\n",
            "71\n",
            "eval: split train. loss 1.169379e-02. error 1.65%. misses: 827\n",
            "eval: split test . loss 2.111713e-02. error 3.17%. misses: 317\n",
            "72\n",
            "eval: split train. loss 1.304817e-02. error 1.84%. misses: 922\n",
            "eval: split test . loss 2.092837e-02. error 3.15%. misses: 315\n",
            "73\n",
            "eval: split train. loss 1.145600e-02. error 1.62%. misses: 810\n",
            "eval: split test . loss 2.027484e-02. error 3.03%. misses: 303\n",
            "74\n",
            "eval: split train. loss 1.082531e-02. error 1.48%. misses: 741\n",
            "eval: split test . loss 2.068331e-02. error 3.13%. misses: 313\n",
            "75\n",
            "eval: split train. loss 1.099375e-02. error 1.55%. misses: 777\n",
            "eval: split test . loss 2.036789e-02. error 2.90%. misses: 289\n",
            "76\n",
            "eval: split train. loss 1.009804e-02. error 1.39%. misses: 694\n",
            "eval: split test . loss 2.052394e-02. error 3.02%. misses: 302\n"
          ]
        }
      ],
      "source": [
        "learning_rate = 0.03\n",
        "\n",
        "# init rng\n",
        "torch.manual_seed(1337)\n",
        "np.random.seed(1337)\n",
        "torch.use_deterministic_algorithms(True)\n",
        "\n",
        "# init a model\n",
        "model = BaseNet()\n",
        "print(\"model stats:\")\n",
        "print(\"# params:      \", sum(p.numel() for p in model.parameters())) # in paper total is 9,760\n",
        "print(\"# MACs:        \", model.macs)\n",
        "print(\"# activations: \", model.acts)\n",
        "\n",
        "# init data\n",
        "Xtr, Ytr = torch.load('trainfull.pt')\n",
        "Xte, Yte = torch.load('testfull.pt')\n",
        "\n",
        "# init optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "def eval_split(split):\n",
        "    # eval the full train/test set, batched implementation for efficiency\n",
        "    model.eval()\n",
        "    X, Y = (Xtr, Ytr) if split == 'train' else (Xte, Yte)\n",
        "    Yhat = model(X)\n",
        "    loss = torch.mean((Y - Yhat)**2)\n",
        "    err = torch.mean((Y.argmax(dim=1) != Yhat.argmax(dim=1)).float())\n",
        "    print(f\"eval: split {split:5s}. loss {loss.item():e}. error {err.item()*100:.2f}%. misses: {int(err.item()*Y.size(0))}\")\n",
        "\n",
        "# train\n",
        "for pass_num in range(80):\n",
        "\n",
        "    # perform one epoch of training\n",
        "    model.train()\n",
        "    for step_num in range(Xtr.size(0)):\n",
        "\n",
        "        # fetch a single example into a batch of 1\n",
        "        x, y = Xtr[[step_num]], Ytr[[step_num]]\n",
        "\n",
        "        # forward the model and the loss\n",
        "        yhat = model(x)\n",
        "        loss = torch.mean((y - yhat)**2)\n",
        "\n",
        "        # calculate the gradient and update the parameters\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # after epoch epoch evaluate the train and test error / metrics\n",
        "    print(pass_num + 1)\n",
        "    eval_split('train')\n",
        "    eval_split('test')\n",
        "\n",
        "# save final model to file\n",
        "torch.save(model.state_dict(), 'base_model_data.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdH2e8h2HsC9"
      },
      "source": [
        "Using the original 1989 model, \n",
        "\n",
        "* with the \"weird\" MSE loss function, \n",
        "* tanh activation, \n",
        "* \"plain\" stochastic gradient descent with a fixed learning rate, \n",
        "* and no regularization, \n",
        "\n",
        "we still see a very respectable increase in performance just by training on more data (and since there is more data, adding more training passes.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOqF2hayIEfG"
      },
      "source": [
        "This would have substantially increased the training time in 1989 - the original model trained on 7291 training samples took 3 days to train, and this training set is about 7x that size - but the inference time would not have been affected."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1aMMBSfVuFr"
      },
      "source": [
        "### Modern model with larger data set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QvhTRhdOodO"
      },
      "source": [
        "Finally, let's see how our \"modern\" updated neural network does when trained on the larger dataset - 50,000 training samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HfsTJg3WLSvk"
      },
      "outputs": [],
      "source": [
        "class ModernNet(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # initialization as described in the paper to my best ability, but it doesn't look right...\n",
        "        winit = lambda fan_in, *shape: (torch.rand(*shape) - 0.5) * 2 * 2.4 / fan_in**0.5\n",
        "        macs = 0 # keep track of MACs (multiply accumulates)\n",
        "        acts = 0 # keep track of number of activations\n",
        "\n",
        "        # H1 layer parameters and their initialization\n",
        "        self.H1w = nn.Parameter(winit(5*5*1, 12, 1, 5, 5))\n",
        "        self.H1b = nn.Parameter(torch.zeros(12, 8, 8)) # presumably init to zero for biases\n",
        "        macs += (5*5*1) * (8*8) * 12\n",
        "        acts += (8*8) * 12\n",
        "\n",
        "        # H2 layer parameters and their initialization\n",
        "        \"\"\"\n",
        "        H2 neurons all connect to only 8 of the 12 input planes, with an unspecified pattern\n",
        "        I am going to assume the most sensible block pattern where 4 planes at a time connect\n",
        "        to differently overlapping groups of 8/12 input planes. We will implement this with 3\n",
        "        separate convolutions that we concatenate the results of.\n",
        "        \"\"\"\n",
        "        self.H2w = nn.Parameter(winit(5*5*8, 12, 8, 5, 5))\n",
        "        self.H2b = nn.Parameter(torch.zeros(12, 4, 4)) # presumably init to zero for biases\n",
        "        macs += (5*5*8) * (4*4) * 12\n",
        "        acts += (4*4) * 12\n",
        "\n",
        "        # H3 is a fully connected layer\n",
        "        self.H3w = nn.Parameter(winit(4*4*12, 4*4*12, 30))\n",
        "        self.H3b = nn.Parameter(torch.zeros(30))\n",
        "        macs += (4*4*12) * 30\n",
        "        acts += 30\n",
        "\n",
        "        # output layer is also fully connected layer\n",
        "        self.outw = nn.Parameter(winit(30, 30, 10))\n",
        "        self.outb = nn.Parameter(torch.zeros(10))\n",
        "        macs += 30 * 10\n",
        "        acts += 10\n",
        "\n",
        "        self.macs = macs\n",
        "        self.acts = acts\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # poor man's data augmentation by 1 pixel along x/y directions\n",
        "        if self.training:\n",
        "            shift_x, shift_y = np.random.randint(-1, 2, size=2)\n",
        "            x = torch.roll(x, (shift_x, shift_y), (2, 3))\n",
        "\n",
        "        # x has shape (1, 1, 16, 16)\n",
        "        x = F.pad(x, (2, 2, 2, 2), 'constant', -1.0) # pad by two using constant -1 for background\n",
        "        x = F.conv2d(x, self.H1w, stride=2) + self.H1b\n",
        "        x = torch.relu(x) # Note: ReLU activation!\n",
        "\n",
        "        # x is now shape (1, 12, 8, 8)\n",
        "        x = F.pad(x, (2, 2, 2, 2), 'constant', -1.0) # pad by two using constant -1 for background\n",
        "        slice1 = F.conv2d(x[:, 0:8], self.H2w[0:4], stride=2) # first 4 planes look at first 8 input planes\n",
        "        slice2 = F.conv2d(x[:, 4:12], self.H2w[4:8], stride=2) # next 4 planes look at last 8 input planes\n",
        "        slice3 = F.conv2d(torch.cat((x[:, 0:4], x[:, 8:12]), dim=1), self.H2w[8:12], stride=2) # last 4 planes are cross\n",
        "        x = torch.cat((slice1, slice2, slice3), dim=1) + self.H2b\n",
        "        x = torch.relu(x) # Note: ReLU activation!\n",
        "\n",
        "        # x is now shape (1, 12, 4, 4)\n",
        "        x = x.flatten(start_dim=1) # (1, 12*4*4)\n",
        "        x = x @ self.H3w + self.H3b\n",
        "        x = torch.relu(x) # Note: ReLU activation!\n",
        "        x = F.dropout(x, p=0.25, training=self.training) # Note: dropout layer!\n",
        "\n",
        "\n",
        "        # x is now shape (1, 30)\n",
        "        x = x @ self.outw + self.outb\n",
        "\n",
        "         # x is finally shape (1, 10)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5efX9bOgV0u2",
        "outputId": "e511c0b7-6f2e-46c5-83e0-e21f2626afe7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model stats:\n",
            "# params:       9760\n",
            "# MACs:         63660\n",
            "# activations:  1000\n",
            "1\n",
            "eval: split train. loss 1.800405e-01. error 5.34%. misses: 2669\n",
            "eval: split test . loss 1.658235e-01. error 4.76%. misses: 476\n",
            "2\n",
            "eval: split train. loss 1.235864e-01. error 3.79%. misses: 1893\n",
            "eval: split test . loss 1.099108e-01. error 3.30%. misses: 329\n",
            "3\n",
            "eval: split train. loss 1.132230e-01. error 3.56%. misses: 1779\n",
            "eval: split test . loss 9.745394e-02. error 3.22%. misses: 322\n",
            "4\n",
            "eval: split train. loss 9.968607e-02. error 2.95%. misses: 1475\n",
            "eval: split test . loss 9.140631e-02. error 2.69%. misses: 269\n",
            "5\n",
            "eval: split train. loss 1.013773e-01. error 2.98%. misses: 1489\n",
            "eval: split test . loss 8.744171e-02. error 2.56%. misses: 255\n",
            "6\n",
            "eval: split train. loss 9.772331e-02. error 2.87%. misses: 1434\n",
            "eval: split test . loss 8.561321e-02. error 2.71%. misses: 271\n",
            "7\n",
            "eval: split train. loss 9.237381e-02. error 2.82%. misses: 1408\n",
            "eval: split test . loss 8.389988e-02. error 2.62%. misses: 262\n",
            "8\n",
            "eval: split train. loss 9.329148e-02. error 2.73%. misses: 1367\n",
            "eval: split test . loss 8.308051e-02. error 2.65%. misses: 264\n",
            "9\n",
            "eval: split train. loss 7.840865e-02. error 2.42%. misses: 1209\n",
            "eval: split test . loss 6.642158e-02. error 2.13%. misses: 212\n",
            "10\n",
            "eval: split train. loss 8.026410e-02. error 2.45%. misses: 1224\n",
            "eval: split test . loss 7.293022e-02. error 2.36%. misses: 236\n",
            "11\n",
            "eval: split train. loss 9.108482e-02. error 2.80%. misses: 1401\n",
            "eval: split test . loss 7.860015e-02. error 2.45%. misses: 244\n",
            "12\n",
            "eval: split train. loss 7.954830e-02. error 2.46%. misses: 1231\n",
            "eval: split test . loss 7.213327e-02. error 2.32%. misses: 231\n",
            "13\n",
            "eval: split train. loss 7.463950e-02. error 2.28%. misses: 1142\n",
            "eval: split test . loss 6.561451e-02. error 2.09%. misses: 208\n",
            "14\n",
            "eval: split train. loss 7.180358e-02. error 2.28%. misses: 1140\n",
            "eval: split test . loss 6.521061e-02. error 2.11%. misses: 210\n",
            "15\n",
            "eval: split train. loss 7.282287e-02. error 2.17%. misses: 1083\n",
            "eval: split test . loss 6.592727e-02. error 1.96%. misses: 196\n",
            "16\n",
            "eval: split train. loss 8.858477e-02. error 2.76%. misses: 1379\n",
            "eval: split test . loss 7.130072e-02. error 2.52%. misses: 252\n",
            "17\n",
            "eval: split train. loss 7.600951e-02. error 2.22%. misses: 1109\n",
            "eval: split test . loss 6.610008e-02. error 2.08%. misses: 208\n",
            "18\n",
            "eval: split train. loss 7.291627e-02. error 2.10%. misses: 1047\n",
            "eval: split test . loss 6.262310e-02. error 1.76%. misses: 175\n",
            "19\n",
            "eval: split train. loss 6.770524e-02. error 2.05%. misses: 1025\n",
            "eval: split test . loss 5.550143e-02. error 1.63%. misses: 163\n",
            "20\n",
            "eval: split train. loss 7.845482e-02. error 2.33%. misses: 1166\n",
            "eval: split test . loss 6.724408e-02. error 1.95%. misses: 195\n",
            "21\n",
            "eval: split train. loss 7.659978e-02. error 2.32%. misses: 1159\n",
            "eval: split test . loss 6.563076e-02. error 2.20%. misses: 219\n",
            "22\n",
            "eval: split train. loss 6.487056e-02. error 1.95%. misses: 975\n",
            "eval: split test . loss 5.405575e-02. error 1.55%. misses: 154\n",
            "23\n",
            "eval: split train. loss 7.666572e-02. error 2.29%. misses: 1145\n",
            "eval: split test . loss 6.522575e-02. error 2.07%. misses: 207\n",
            "24\n",
            "eval: split train. loss 7.719738e-02. error 2.22%. misses: 1111\n",
            "eval: split test . loss 6.901471e-02. error 2.03%. misses: 203\n",
            "25\n",
            "eval: split train. loss 7.369056e-02. error 2.21%. misses: 1107\n",
            "eval: split test . loss 6.501772e-02. error 1.84%. misses: 184\n",
            "26\n",
            "eval: split train. loss 6.239363e-02. error 1.94%. misses: 968\n",
            "eval: split test . loss 5.587996e-02. error 1.67%. misses: 166\n",
            "27\n",
            "eval: split train. loss 6.331309e-02. error 1.93%. misses: 966\n",
            "eval: split test . loss 5.884980e-02. error 1.82%. misses: 182\n",
            "28\n",
            "eval: split train. loss 7.507955e-02. error 2.35%. misses: 1174\n",
            "eval: split test . loss 6.500146e-02. error 2.05%. misses: 205\n",
            "29\n",
            "eval: split train. loss 7.138236e-02. error 2.15%. misses: 1075\n",
            "eval: split test . loss 5.975653e-02. error 2.00%. misses: 199\n",
            "30\n",
            "eval: split train. loss 6.826258e-02. error 1.98%. misses: 989\n",
            "eval: split test . loss 6.064120e-02. error 1.86%. misses: 186\n",
            "31\n",
            "eval: split train. loss 6.137656e-02. error 1.78%. misses: 889\n",
            "eval: split test . loss 5.290557e-02. error 1.59%. misses: 159\n",
            "32\n",
            "eval: split train. loss 6.572439e-02. error 2.06%. misses: 1028\n",
            "eval: split test . loss 5.870391e-02. error 1.88%. misses: 187\n",
            "33\n",
            "eval: split train. loss 6.773707e-02. error 2.05%. misses: 1023\n",
            "eval: split test . loss 5.926559e-02. error 1.91%. misses: 190\n",
            "34\n",
            "eval: split train. loss 7.635399e-02. error 2.36%. misses: 1178\n",
            "eval: split test . loss 6.279826e-02. error 2.00%. misses: 199\n",
            "35\n",
            "eval: split train. loss 6.204426e-02. error 1.83%. misses: 915\n",
            "eval: split test . loss 5.545245e-02. error 1.81%. misses: 181\n",
            "36\n",
            "eval: split train. loss 7.754768e-02. error 2.27%. misses: 1137\n",
            "eval: split test . loss 6.678746e-02. error 2.03%. misses: 203\n",
            "37\n",
            "eval: split train. loss 5.733868e-02. error 1.77%. misses: 882\n",
            "eval: split test . loss 5.076264e-02. error 1.58%. misses: 157\n",
            "38\n",
            "eval: split train. loss 6.474368e-02. error 1.90%. misses: 949\n",
            "eval: split test . loss 5.606046e-02. error 1.68%. misses: 167\n",
            "39\n",
            "eval: split train. loss 6.462176e-02. error 2.01%. misses: 1006\n",
            "eval: split test . loss 5.610524e-02. error 1.76%. misses: 175\n",
            "40\n",
            "eval: split train. loss 6.236600e-02. error 1.88%. misses: 939\n",
            "eval: split test . loss 5.314400e-02. error 1.49%. misses: 148\n",
            "41\n",
            "eval: split train. loss 6.110935e-02. error 1.87%. misses: 932\n",
            "eval: split test . loss 4.973492e-02. error 1.60%. misses: 160\n",
            "42\n",
            "eval: split train. loss 6.257211e-02. error 1.72%. misses: 860\n",
            "eval: split test . loss 5.584625e-02. error 1.55%. misses: 154\n",
            "43\n",
            "eval: split train. loss 5.855170e-02. error 1.79%. misses: 892\n",
            "eval: split test . loss 5.095017e-02. error 1.72%. misses: 172\n",
            "44\n",
            "eval: split train. loss 6.403045e-02. error 2.00%. misses: 999\n",
            "eval: split test . loss 5.793996e-02. error 1.79%. misses: 178\n",
            "45\n",
            "eval: split train. loss 6.075210e-02. error 1.77%. misses: 884\n",
            "eval: split test . loss 5.380621e-02. error 1.71%. misses: 171\n",
            "46\n",
            "eval: split train. loss 5.586766e-02. error 1.73%. misses: 863\n",
            "eval: split test . loss 4.786943e-02. error 1.38%. misses: 137\n",
            "47\n",
            "eval: split train. loss 5.750018e-02. error 1.74%. misses: 870\n",
            "eval: split test . loss 5.099117e-02. error 1.58%. misses: 157\n",
            "48\n",
            "eval: split train. loss 6.201304e-02. error 1.91%. misses: 956\n",
            "eval: split test . loss 5.117364e-02. error 1.73%. misses: 173\n",
            "49\n",
            "eval: split train. loss 5.527091e-02. error 1.67%. misses: 836\n",
            "eval: split test . loss 4.755578e-02. error 1.47%. misses: 147\n",
            "50\n",
            "eval: split train. loss 6.251565e-02. error 1.89%. misses: 942\n",
            "eval: split test . loss 5.431730e-02. error 1.67%. misses: 166\n",
            "51\n",
            "eval: split train. loss 5.895694e-02. error 1.74%. misses: 868\n",
            "eval: split test . loss 4.977573e-02. error 1.49%. misses: 148\n",
            "52\n",
            "eval: split train. loss 5.973049e-02. error 1.90%. misses: 951\n",
            "eval: split test . loss 5.299607e-02. error 1.72%. misses: 172\n",
            "53\n",
            "eval: split train. loss 5.699369e-02. error 1.76%. misses: 880\n",
            "eval: split test . loss 5.072964e-02. error 1.67%. misses: 166\n",
            "54\n",
            "eval: split train. loss 6.304983e-02. error 1.84%. misses: 918\n",
            "eval: split test . loss 5.321763e-02. error 1.59%. misses: 159\n",
            "55\n",
            "eval: split train. loss 6.556917e-02. error 2.02%. misses: 1011\n",
            "eval: split test . loss 6.161071e-02. error 1.84%. misses: 184\n",
            "56\n",
            "eval: split train. loss 5.485301e-02. error 1.69%. misses: 846\n",
            "eval: split test . loss 4.799609e-02. error 1.50%. misses: 149\n",
            "57\n",
            "eval: split train. loss 5.543558e-02. error 1.64%. misses: 822\n",
            "eval: split test . loss 4.784161e-02. error 1.49%. misses: 148\n",
            "58\n",
            "eval: split train. loss 5.596628e-02. error 1.73%. misses: 867\n",
            "eval: split test . loss 4.869924e-02. error 1.50%. misses: 149\n",
            "59\n",
            "eval: split train. loss 5.378242e-02. error 1.68%. misses: 837\n",
            "eval: split test . loss 4.986404e-02. error 1.62%. misses: 162\n",
            "60\n",
            "eval: split train. loss 5.661343e-02. error 1.75%. misses: 874\n",
            "eval: split test . loss 4.837556e-02. error 1.45%. misses: 144\n",
            "61\n",
            "eval: split train. loss 6.158134e-02. error 1.83%. misses: 917\n",
            "eval: split test . loss 5.308961e-02. error 1.52%. misses: 152\n",
            "62\n",
            "eval: split train. loss 5.769319e-02. error 1.79%. misses: 894\n",
            "eval: split test . loss 4.892448e-02. error 1.62%. misses: 162\n",
            "63\n",
            "eval: split train. loss 5.793946e-02. error 1.79%. misses: 896\n",
            "eval: split test . loss 5.130624e-02. error 1.51%. misses: 151\n",
            "64\n",
            "eval: split train. loss 6.359904e-02. error 1.94%. misses: 968\n",
            "eval: split test . loss 5.543852e-02. error 1.59%. misses: 159\n",
            "65\n",
            "eval: split train. loss 5.666271e-02. error 1.67%. misses: 836\n",
            "eval: split test . loss 4.847063e-02. error 1.43%. misses: 142\n",
            "66\n",
            "eval: split train. loss 5.674731e-02. error 1.75%. misses: 875\n",
            "eval: split test . loss 4.871766e-02. error 1.60%. misses: 160\n",
            "67\n",
            "eval: split train. loss 5.627377e-02. error 1.67%. misses: 834\n",
            "eval: split test . loss 4.933117e-02. error 1.58%. misses: 157\n",
            "68\n",
            "eval: split train. loss 5.467863e-02. error 1.66%. misses: 829\n",
            "eval: split test . loss 4.582687e-02. error 1.51%. misses: 151\n",
            "69\n",
            "eval: split train. loss 5.193768e-02. error 1.55%. misses: 773\n",
            "eval: split test . loss 4.574114e-02. error 1.35%. misses: 135\n",
            "70\n",
            "eval: split train. loss 5.202961e-02. error 1.63%. misses: 813\n",
            "eval: split test . loss 4.718123e-02. error 1.55%. misses: 154\n",
            "71\n",
            "eval: split train. loss 5.079662e-02. error 1.56%. misses: 780\n",
            "eval: split test . loss 4.439849e-02. error 1.49%. misses: 148\n",
            "72\n",
            "eval: split train. loss 5.294628e-02. error 1.63%. misses: 817\n",
            "eval: split test . loss 4.388249e-02. error 1.40%. misses: 140\n",
            "73\n",
            "eval: split train. loss 5.498884e-02. error 1.72%. misses: 860\n",
            "eval: split test . loss 4.618821e-02. error 1.49%. misses: 148\n",
            "74\n",
            "eval: split train. loss 5.391493e-02. error 1.61%. misses: 805\n",
            "eval: split test . loss 4.802698e-02. error 1.54%. misses: 153\n",
            "75\n",
            "eval: split train. loss 4.984960e-02. error 1.49%. misses: 743\n",
            "eval: split test . loss 4.356157e-02. error 1.34%. misses: 133\n",
            "76\n",
            "eval: split train. loss 5.023967e-02. error 1.56%. misses: 781\n",
            "eval: split test . loss 4.450185e-02. error 1.49%. misses: 148\n",
            "77\n",
            "eval: split train. loss 5.515992e-02. error 1.66%. misses: 832\n",
            "eval: split test . loss 4.790904e-02. error 1.46%. misses: 146\n",
            "78\n",
            "eval: split train. loss 5.488897e-02. error 1.64%. misses: 820\n",
            "eval: split test . loss 4.699790e-02. error 1.51%. misses: 151\n",
            "79\n",
            "eval: split train. loss 5.152977e-02. error 1.58%. misses: 789\n",
            "eval: split test . loss 4.335960e-02. error 1.43%. misses: 142\n",
            "80\n",
            "eval: split train. loss 5.176624e-02. error 1.68%. misses: 841\n",
            "eval: split test . loss 4.423201e-02. error 1.45%. misses: 144\n"
          ]
        }
      ],
      "source": [
        "# The following code is adapted from \"modern.py\" in the repository.\n",
        "\n",
        "learning_rate = 3e-4\n",
        "\n",
        "# init rng\n",
        "torch.manual_seed(1337)\n",
        "np.random.seed(1337)\n",
        "torch.use_deterministic_algorithms(True)\n",
        "\n",
        "# init a model\n",
        "model = ModernNet()\n",
        "print(\"model stats:\")\n",
        "print(\"# params:      \", sum(p.numel() for p in model.parameters())) # in paper total is 9,760\n",
        "print(\"# MACs:        \", model.macs)\n",
        "print(\"# activations: \", model.acts)\n",
        "\n",
        "# init data\n",
        "Xtr, Ytr = torch.load('trainfull.pt')\n",
        "Xte, Yte = torch.load('testfull.pt')\n",
        "\n",
        "# init optimizer\n",
        "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "def eval_split(split):\n",
        "    # eval the full train/test set, batched implementation for efficiency\n",
        "    model.eval()\n",
        "    X, Y = (Xtr, Ytr) if split == 'train' else (Xte, Yte)\n",
        "    Yhat = model(X)\n",
        "    loss = F.cross_entropy(Yhat, Y.argmax(dim=1))\n",
        "    err = torch.mean((Y.argmax(dim=1) != Yhat.argmax(dim=1)).float())\n",
        "    print(f\"eval: split {split:5s}. loss {loss.item():e}. error {err.item()*100:.2f}%. misses: {int(err.item()*Y.size(0))}\")\n",
        "  \n",
        "# train\n",
        "for pass_num in range(80):\n",
        "\n",
        "    # learning rate decay\n",
        "    alpha = pass_num / 79\n",
        "    for g in optimizer.param_groups:\n",
        "        g['lr'] = (1 - alpha) * learning_rate + alpha * (learning_rate / 3)\n",
        "\n",
        "    # perform one epoch of training\n",
        "    model.train()\n",
        "    for step_num in range(Xtr.size(0)):\n",
        "\n",
        "        # fetch a single example into a batch of 1\n",
        "        x, y = Xtr[[step_num]], Ytr[[step_num]]\n",
        "\n",
        "        # forward the model and the loss\n",
        "        yhat = model(x)\n",
        "        loss = F.cross_entropy(yhat, y.argmax(dim=1))\n",
        "\n",
        "        # calculate the gradient and update the parameters\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # after epoch epoch evaluate the train and test error / metrics\n",
        "    print(pass_num + 1)\n",
        "    eval_split('train')\n",
        "    eval_split('test')\n",
        "\n",
        "# save final model to file\n",
        "torch.save(model.state_dict(), 'modern_model_data.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoVCR0gj92E_"
      },
      "source": [
        "With the extra data, our \"updated\" version of the 1989 model, with\n",
        "\n",
        "* MSE loss changed to cross entropy loss \n",
        "* tanh activation changed to ReLU\n",
        "* SGD with constant learning rate changed to Adam with learning rate decay\n",
        "* added dropout layer for regularization\n",
        "* basic data augmentation (shift up to 1 pixel in each direction)\n",
        "\n",
        "but the same basic structure, easily achieves less than 2% error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4J-d1RVHr0H"
      },
      "source": [
        "## Summary of results\n",
        "\n",
        "To summarize, the test error rate of each model was:\n",
        "\n",
        "* Original 1989 paper: **5.0%**\n",
        "* Pytorch model based on original: **4.14%**\n",
        "* ... + cross entropy loss: **4.58%**\n",
        "* ... + Adam optimizer: **3.89%**\n",
        "* ... + data augmentation: **2.29%**\n",
        "* ... + dropout + ReLU activations: **2.09%**\n",
        "* ... + more training data: **1.45%**\n",
        "\n",
        "and, \n",
        "\n",
        "* Pytorch model based on 1989 original + more training data: about **3%**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
