{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVOiBENthzmi"
      },
      "source": [
        "### Retrieve and preprocess data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYJFw1dXKQt2"
      },
      "source": [
        "The original paper explains how the handwritten digit dataset was acquired and preprocessed:\n",
        "\n",
        "> **2.1 Data Base.** The data base used to train and test the networks consists of 9298 segmented numerals digitized from handwritten zip codes that appeared on U.S. mail passing through the Buffalo, NY post office. Examples of such images are shown in Figure 1. The digits were written by many different people, using a great variety of sizes, writing styles, and instruments, with widely varying amounts of care; 7291 examples are used for training the network and 2007 are used for testing the generalization performance. One important feature of this data base is that both the training set and the testing set contain numerous examples that are ambiguous, unclassifiable, or even misclassified.\n",
        "> \n",
        "> **2.2 Preprocessing.** Locating the zip code on the envelope and separating each digit from its neighbors, a very hard task in itself, was performed by Postal Service contractors (Wang and Srihari 1988). At this point, the size of a digit image varies but is typically around 40 by 60 pixels. A linear transformation is then applied to make the image fit in a 16 by 16 pixel image. This transformation preserves the aspect ratio of the character, and is performed after extraneous marks in the image have been removed. Because of the linear transformation, the resulting image is not binary but has multiple gray levels, since a variable number of pixels in the original image can fall into a given pixel in the target image. The gray levels of each image are scaled and translated to fall within the range -1 to 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2Y4dBAsyP86"
      },
      "source": [
        "Since we don't have this exact data set, we take MNIST - a very similar data set! - and use it as an approximation of the original data set. MNIST is different in a few ways.\n",
        "\n",
        "* has 50,000 training samples, and 10,000 test samples (compared to 7291 and 2007 respectively, for the original paper), \n",
        "* and each sample is 28x28 pixels (compared to 16x16 for the original)\n",
        "\n",
        "so to \"simulate\" the original, we sample without replacement the correct number of training and test samples from the complete MNIST and scale them down to the smaller 16x16 size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hPAZoOMysE2"
      },
      "source": [
        "---\n",
        "\n",
        "ðŸ›‘ Since our data is not exactly the same as the original paper, the numeric results may also be slightly different.\n",
        "\n",
        "ðŸ§ªTo explore how sensitive the numeric result is to the specific draw of training and test samples, you can try changing the seeds in the following cell and re-running!\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJqRxqAdelLg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from torchvision import datasets\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLkI1Q9lh2X7",
        "outputId": "d6a46397-6e82-4de6-bf40-0de145176ad1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9912422/9912422 [00:00<00:00, 92820537.92it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28881/28881 [00:00<00:00, 28001778.51it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1648877/1648877 [00:00<00:00, 25814812.01it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4542/4542 [00:00<00:00, 12906862.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# The following code is adapted from \"prepro.py\" in the repository.\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "np.random.seed(1337)\n",
        "\n",
        "for split in {'train', 'test'}:\n",
        "\n",
        "    data = datasets.MNIST('./data', train=split=='train', download=True)\n",
        "\n",
        "    n = 7291 if split == 'train' else 2007\n",
        "    rp = np.random.permutation(len(data))[:n]\n",
        "\n",
        "    X = torch.full((n, 1, 16, 16), 0.0, dtype=torch.float32)\n",
        "    Y = torch.full((n, 10), -1.0, dtype=torch.float32)\n",
        "    for i, ix in enumerate(rp):\n",
        "        I, yint = data[int(ix)]\n",
        "        # PIL image -> numpy -> torch tensor -> [-1, 1] fp32\n",
        "        xi = torch.from_numpy(np.array(I, dtype=np.float32)) / 127.5 - 1.0\n",
        "        # add a fake batch dimension and a channel dimension of 1 or F.interpolate won't be happy\n",
        "        xi = xi[None, None, ...]\n",
        "        # resize to (16, 16) images with bilinear interpolation\n",
        "        xi = F.interpolate(xi, (16, 16), mode='bilinear')\n",
        "        X[i] = xi[0] # store\n",
        "\n",
        "        # set the correct class to have target of +1.0\n",
        "        Y[i, yint] = 1.0\n",
        "\n",
        "    torch.save((X, Y), split + '1989.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4zUJgNd4a9O"
      },
      "source": [
        "The training and test datasets are now saved as 'train1989.pt' and 'test1989.pt', respectively.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
