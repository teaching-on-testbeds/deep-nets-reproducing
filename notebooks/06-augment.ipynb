{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RW7kF3GsJyf5"
      },
      "source": [
        "### Training with data augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a91fSklsk9yO"
      },
      "source": [
        "To combat potential overfitting, we can add some basic data augmentation. When we load a training sample, we will randomly shift it by up to one pixel in the vertical and/or horizontal directions.\n",
        "\n",
        "We also increase the number of passes again to 60, since data augmentation (1) increases the effective \"size\" of the training set, and (2) also adds some \"noise\" to the training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJqRxqAdelLg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UE0iOABZKH2Y"
      },
      "outputs": [],
      "source": [
        "# The following code is adapted from \"modern.py\" in the repository.\n",
        "\n",
        "class ModernDataAugNet(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # initialization as described in the paper to my best ability, but it doesn't look right...\n",
        "        winit = lambda fan_in, *shape: (torch.rand(*shape) - 0.5) * 2 * 2.4 / fan_in**0.5\n",
        "        macs = 0 # keep track of MACs (multiply accumulates)\n",
        "        acts = 0 # keep track of number of activations\n",
        "\n",
        "        # H1 layer parameters and their initialization\n",
        "        self.H1w = nn.Parameter(winit(5*5*1, 12, 1, 5, 5))\n",
        "        self.H1b = nn.Parameter(torch.zeros(12, 8, 8)) # presumably init to zero for biases\n",
        "        macs += (5*5*1) * (8*8) * 12\n",
        "        acts += (8*8) * 12\n",
        "\n",
        "        # H2 layer parameters and their initialization\n",
        "        \"\"\"\n",
        "        H2 neurons all connect to only 8 of the 12 input planes, with an unspecified pattern\n",
        "        I am going to assume the most sensible block pattern where 4 planes at a time connect\n",
        "        to differently overlapping groups of 8/12 input planes. We will implement this with 3\n",
        "        separate convolutions that we concatenate the results of.\n",
        "        \"\"\"\n",
        "        self.H2w = nn.Parameter(winit(5*5*8, 12, 8, 5, 5))\n",
        "        self.H2b = nn.Parameter(torch.zeros(12, 4, 4)) # presumably init to zero for biases\n",
        "        macs += (5*5*8) * (4*4) * 12\n",
        "        acts += (4*4) * 12\n",
        "\n",
        "        # H3 is a fully connected layer\n",
        "        self.H3w = nn.Parameter(winit(4*4*12, 4*4*12, 30))\n",
        "        self.H3b = nn.Parameter(torch.zeros(30))\n",
        "        macs += (4*4*12) * 30\n",
        "        acts += 30\n",
        "\n",
        "        # output layer is also fully connected layer\n",
        "        self.outw = nn.Parameter(winit(30, 30, 10))\n",
        "        self.outb = nn.Parameter(torch.zeros(10))\n",
        "        macs += 30 * 10\n",
        "        acts += 10\n",
        "\n",
        "        self.macs = macs\n",
        "        self.acts = acts\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Note: basic data augmentation by 1 pixel along x/y directions\n",
        "        if self.training:\n",
        "            shift_x, shift_y = np.random.randint(-1, 2, size=2)\n",
        "            x = torch.roll(x, (shift_x, shift_y), (2, 3))\n",
        "\n",
        "        # x has shape (1, 1, 16, 16)\n",
        "        x = F.pad(x, (2, 2, 2, 2), 'constant', -1.0) # pad by two using constant -1 for background\n",
        "        x = F.conv2d(x, self.H1w, stride=2) + self.H1b\n",
        "        x = torch.tanh(x)\n",
        "\n",
        "        # x is now shape (1, 12, 8, 8)\n",
        "        x = F.pad(x, (2, 2, 2, 2), 'constant', -1.0) # pad by two using constant -1 for background\n",
        "        slice1 = F.conv2d(x[:, 0:8], self.H2w[0:4], stride=2) # first 4 planes look at first 8 input planes\n",
        "        slice2 = F.conv2d(x[:, 4:12], self.H2w[4:8], stride=2) # next 4 planes look at last 8 input planes\n",
        "        slice3 = F.conv2d(torch.cat((x[:, 0:4], x[:, 8:12]), dim=1), self.H2w[8:12], stride=2) # last 4 planes are cross\n",
        "        x = torch.cat((slice1, slice2, slice3), dim=1) + self.H2b\n",
        "        x = torch.tanh(x)\n",
        "\n",
        "        # x is now shape (1, 12, 4, 4)\n",
        "        x = x.flatten(start_dim=1) # (1, 12*4*4)\n",
        "        x = x @ self.H3w + self.H3b\n",
        "        x = torch.tanh(x)\n",
        "\n",
        "        # x is now shape (1, 30)\n",
        "        x = x @ self.outw + self.outb\n",
        "\n",
        "         # x is finally shape (1, 10)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzcQguWGN9Lw",
        "outputId": "bcc745de-1bac-40c0-fcd3-1c2b2b9bbdd5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model stats:\n",
            "# params:       9760\n",
            "# MACs:         63660\n",
            "# activations:  1000\n",
            "1\n",
            "eval: split train. loss 5.288423e-01. error 13.48%. misses: 982\n",
            "eval: split test . loss 5.376812e-01. error 13.65%. misses: 274\n",
            "2\n",
            "eval: split train. loss 3.618094e-01. error 9.74%. misses: 710\n",
            "eval: split test . loss 3.622440e-01. error 10.21%. misses: 204\n",
            "3\n",
            "eval: split train. loss 2.770010e-01. error 7.78%. misses: 567\n",
            "eval: split test . loss 2.691664e-01. error 7.77%. misses: 155\n",
            "4\n",
            "eval: split train. loss 2.191166e-01. error 6.32%. misses: 460\n",
            "eval: split test . loss 2.131962e-01. error 5.88%. misses: 117\n",
            "5\n",
            "eval: split train. loss 1.949559e-01. error 5.71%. misses: 416\n",
            "eval: split test . loss 2.028036e-01. error 6.13%. misses: 122\n",
            "6\n",
            "eval: split train. loss 1.760966e-01. error 5.09%. misses: 371\n",
            "eval: split test . loss 1.767326e-01. error 5.28%. misses: 105\n",
            "7\n",
            "eval: split train. loss 1.579356e-01. error 4.69%. misses: 341\n",
            "eval: split test . loss 1.632146e-01. error 5.08%. misses: 102\n",
            "8\n",
            "eval: split train. loss 1.546873e-01. error 4.65%. misses: 339\n",
            "eval: split test . loss 1.586833e-01. error 5.38%. misses: 107\n",
            "9\n",
            "eval: split train. loss 1.436166e-01. error 4.13%. misses: 301\n",
            "eval: split test . loss 1.457766e-01. error 4.19%. misses: 84\n",
            "10\n",
            "eval: split train. loss 1.445486e-01. error 4.10%. misses: 298\n",
            "eval: split test . loss 1.534751e-01. error 4.78%. misses: 95\n",
            "11\n",
            "eval: split train. loss 1.282287e-01. error 3.80%. misses: 277\n",
            "eval: split test . loss 1.302226e-01. error 3.54%. misses: 71\n",
            "12\n",
            "eval: split train. loss 1.162532e-01. error 3.46%. misses: 252\n",
            "eval: split test . loss 1.203833e-01. error 3.69%. misses: 74\n",
            "13\n",
            "eval: split train. loss 1.195214e-01. error 3.39%. misses: 247\n",
            "eval: split test . loss 1.223767e-01. error 3.79%. misses: 76\n",
            "14\n",
            "eval: split train. loss 1.282652e-01. error 3.80%. misses: 277\n",
            "eval: split test . loss 1.363554e-01. error 4.29%. misses: 86\n",
            "15\n",
            "eval: split train. loss 1.155965e-01. error 3.48%. misses: 253\n",
            "eval: split test . loss 1.240504e-01. error 4.04%. misses: 81\n",
            "16\n",
            "eval: split train. loss 1.248292e-01. error 3.77%. misses: 274\n",
            "eval: split test . loss 1.281881e-01. error 3.89%. misses: 77\n",
            "17\n",
            "eval: split train. loss 1.087283e-01. error 3.26%. misses: 237\n",
            "eval: split test . loss 1.133182e-01. error 4.04%. misses: 81\n",
            "18\n",
            "eval: split train. loss 1.060524e-01. error 3.31%. misses: 241\n",
            "eval: split test . loss 1.095671e-01. error 3.59%. misses: 72\n",
            "19\n",
            "eval: split train. loss 1.018783e-01. error 2.77%. misses: 201\n",
            "eval: split test . loss 1.119647e-01. error 3.54%. misses: 71\n",
            "20\n",
            "eval: split train. loss 1.059441e-01. error 3.15%. misses: 229\n",
            "eval: split test . loss 1.105624e-01. error 3.24%. misses: 64\n",
            "21\n",
            "eval: split train. loss 9.508307e-02. error 3.02%. misses: 219\n",
            "eval: split test . loss 1.018330e-01. error 3.09%. misses: 62\n",
            "22\n",
            "eval: split train. loss 9.896746e-02. error 3.17%. misses: 231\n",
            "eval: split test . loss 1.078555e-01. error 3.49%. misses: 69\n",
            "23\n",
            "eval: split train. loss 9.296913e-02. error 2.73%. misses: 199\n",
            "eval: split test . loss 9.956329e-02. error 2.89%. misses: 57\n",
            "24\n",
            "eval: split train. loss 9.037106e-02. error 2.72%. misses: 197\n",
            "eval: split test . loss 9.971087e-02. error 3.64%. misses: 72\n",
            "25\n",
            "eval: split train. loss 9.541635e-02. error 2.96%. misses: 216\n",
            "eval: split test . loss 9.714390e-02. error 2.99%. misses: 60\n",
            "26\n",
            "eval: split train. loss 8.974429e-02. error 2.85%. misses: 208\n",
            "eval: split test . loss 9.714546e-02. error 3.29%. misses: 66\n",
            "27\n",
            "eval: split train. loss 9.691910e-02. error 3.13%. misses: 228\n",
            "eval: split test . loss 1.062865e-01. error 3.54%. misses: 71\n",
            "28\n",
            "eval: split train. loss 9.209240e-02. error 2.77%. misses: 201\n",
            "eval: split test . loss 9.495598e-02. error 3.09%. misses: 62\n",
            "29\n",
            "eval: split train. loss 9.879281e-02. error 3.06%. misses: 223\n",
            "eval: split test . loss 1.049158e-01. error 3.54%. misses: 71\n",
            "30\n",
            "eval: split train. loss 8.013640e-02. error 2.44%. misses: 177\n",
            "eval: split test . loss 8.712274e-02. error 2.79%. misses: 56\n",
            "31\n",
            "eval: split train. loss 8.554635e-02. error 2.66%. misses: 193\n",
            "eval: split test . loss 9.827569e-02. error 3.54%. misses: 71\n",
            "32\n",
            "eval: split train. loss 8.692322e-02. error 2.73%. misses: 199\n",
            "eval: split test . loss 8.911619e-02. error 3.09%. misses: 62\n",
            "33\n",
            "eval: split train. loss 7.489812e-02. error 2.26%. misses: 165\n",
            "eval: split test . loss 7.961068e-02. error 2.59%. misses: 52\n",
            "34\n",
            "eval: split train. loss 7.969449e-02. error 2.41%. misses: 176\n",
            "eval: split test . loss 9.192310e-02. error 3.09%. misses: 62\n",
            "35\n",
            "eval: split train. loss 8.791106e-02. error 2.74%. misses: 200\n",
            "eval: split test . loss 9.361415e-02. error 3.44%. misses: 69\n",
            "36\n",
            "eval: split train. loss 8.169266e-02. error 2.69%. misses: 196\n",
            "eval: split test . loss 9.087693e-02. error 3.14%. misses: 62\n",
            "37\n",
            "eval: split train. loss 8.223043e-02. error 2.70%. misses: 196\n",
            "eval: split test . loss 8.681621e-02. error 2.94%. misses: 58\n",
            "38\n",
            "eval: split train. loss 8.433945e-02. error 2.77%. misses: 201\n",
            "eval: split test . loss 8.475801e-02. error 2.74%. misses: 54\n",
            "39\n",
            "eval: split train. loss 7.886920e-02. error 2.48%. misses: 181\n",
            "eval: split test . loss 9.483960e-02. error 3.14%. misses: 62\n",
            "40\n",
            "eval: split train. loss 8.299025e-02. error 2.55%. misses: 185\n",
            "eval: split test . loss 9.049780e-02. error 2.89%. misses: 57\n",
            "41\n",
            "eval: split train. loss 8.681884e-02. error 2.73%. misses: 199\n",
            "eval: split test . loss 9.675436e-02. error 3.29%. misses: 66\n",
            "42\n",
            "eval: split train. loss 6.955151e-02. error 2.14%. misses: 156\n",
            "eval: split test . loss 8.768848e-02. error 2.79%. misses: 56\n",
            "43\n",
            "eval: split train. loss 8.520136e-02. error 2.66%. misses: 193\n",
            "eval: split test . loss 9.470112e-02. error 2.99%. misses: 60\n",
            "44\n",
            "eval: split train. loss 7.159812e-02. error 2.18%. misses: 158\n",
            "eval: split test . loss 8.304404e-02. error 2.59%. misses: 52\n",
            "45\n",
            "eval: split train. loss 6.552258e-02. error 2.17%. misses: 157\n",
            "eval: split test . loss 6.739190e-02. error 2.34%. misses: 47\n",
            "46\n",
            "eval: split train. loss 7.286435e-02. error 2.26%. misses: 165\n",
            "eval: split test . loss 8.435986e-02. error 2.64%. misses: 52\n",
            "47\n",
            "eval: split train. loss 7.106221e-02. error 2.08%. misses: 152\n",
            "eval: split test . loss 8.492766e-02. error 2.84%. misses: 57\n",
            "48\n",
            "eval: split train. loss 7.073149e-02. error 2.00%. misses: 145\n",
            "eval: split test . loss 8.464671e-02. error 2.79%. misses: 56\n",
            "49\n",
            "eval: split train. loss 6.227521e-02. error 2.00%. misses: 145\n",
            "eval: split test . loss 7.101507e-02. error 2.19%. misses: 43\n",
            "50\n",
            "eval: split train. loss 7.140759e-02. error 2.28%. misses: 165\n",
            "eval: split test . loss 7.367770e-02. error 2.24%. misses: 44\n",
            "51\n",
            "eval: split train. loss 7.048882e-02. error 2.39%. misses: 173\n",
            "eval: split test . loss 7.467531e-02. error 2.24%. misses: 44\n",
            "52\n",
            "eval: split train. loss 6.115715e-02. error 1.78%. misses: 130\n",
            "eval: split test . loss 7.857246e-02. error 2.59%. misses: 52\n",
            "53\n",
            "eval: split train. loss 6.965929e-02. error 2.24%. misses: 162\n",
            "eval: split test . loss 7.686994e-02. error 2.59%. misses: 52\n",
            "54\n",
            "eval: split train. loss 6.503507e-02. error 2.07%. misses: 150\n",
            "eval: split test . loss 7.360506e-02. error 2.34%. misses: 47\n",
            "55\n",
            "eval: split train. loss 5.798612e-02. error 1.84%. misses: 134\n",
            "eval: split test . loss 7.086553e-02. error 2.34%. misses: 47\n",
            "56\n",
            "eval: split train. loss 6.643334e-02. error 2.15%. misses: 157\n",
            "eval: split test . loss 7.341871e-02. error 2.19%. misses: 43\n",
            "57\n",
            "eval: split train. loss 6.505820e-02. error 2.04%. misses: 149\n",
            "eval: split test . loss 6.975470e-02. error 2.24%. misses: 44\n",
            "58\n",
            "eval: split train. loss 6.110303e-02. error 1.87%. misses: 135\n",
            "eval: split test . loss 6.950998e-02. error 1.89%. misses: 38\n",
            "59\n",
            "eval: split train. loss 6.753764e-02. error 2.13%. misses: 154\n",
            "eval: split test . loss 8.551276e-02. error 2.94%. misses: 58\n",
            "60\n",
            "eval: split train. loss 6.353480e-02. error 1.89%. misses: 138\n",
            "eval: split test . loss 7.454804e-02. error 2.29%. misses: 46\n"
          ]
        }
      ],
      "source": [
        "# The following code is adapted from \"modern.py\" in the repository.\n",
        "\n",
        "learning_rate = 3e-4\n",
        "\n",
        "# init rng\n",
        "torch.manual_seed(1337)\n",
        "np.random.seed(1337)\n",
        "torch.use_deterministic_algorithms(True)\n",
        "\n",
        "# init a model\n",
        "model = ModernDataAugNet()\n",
        "print(\"model stats:\")\n",
        "print(\"# params:      \", sum(p.numel() for p in model.parameters())) # in paper total is 9,760\n",
        "print(\"# MACs:        \", model.macs)\n",
        "print(\"# activations: \", model.acts)\n",
        "\n",
        "# init data\n",
        "Xtr, Ytr = torch.load('train1989.pt')\n",
        "Xte, Yte = torch.load('test1989.pt')\n",
        "\n",
        "# init optimizer\n",
        "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "def eval_split(split):\n",
        "    # eval the full train/test set, batched implementation for efficiency\n",
        "    model.eval()\n",
        "    X, Y = (Xtr, Ytr) if split == 'train' else (Xte, Yte)\n",
        "    Yhat = model(X)\n",
        "    loss = F.cross_entropy(Yhat, Y.argmax(dim=1))\n",
        "    err = torch.mean((Y.argmax(dim=1) != Yhat.argmax(dim=1)).float())\n",
        "    print(f\"eval: split {split:5s}. loss {loss.item():e}. error {err.item()*100:.2f}%. misses: {int(err.item()*Y.size(0))}\")\n",
        "  \n",
        "# train\n",
        "for pass_num in range(60):\n",
        "\n",
        "    # learning rate decay\n",
        "    alpha = pass_num / 79\n",
        "    for g in optimizer.param_groups:\n",
        "        g['lr'] = (1 - alpha) * learning_rate + alpha * (learning_rate / 3)\n",
        "\n",
        "    # perform one epoch of training\n",
        "    model.train()\n",
        "    for step_num in range(Xtr.size(0)):\n",
        "\n",
        "        # fetch a single example into a batch of 1\n",
        "        x, y = Xtr[[step_num]], Ytr[[step_num]]\n",
        "\n",
        "        # forward the model and the loss\n",
        "        yhat = model(x)\n",
        "        loss = F.cross_entropy(yhat, y.argmax(dim=1))\n",
        "\n",
        "        # calculate the gradient and update the parameters\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # after epoch epoch evaluate the train and test error / metrics\n",
        "    print(pass_num + 1)\n",
        "    eval_split('train')\n",
        "    eval_split('test')\n",
        "\n",
        "# save final model to file\n",
        "torch.save(model.state_dict(), 'dataug_model.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMVNOnkalYvJ"
      },
      "source": [
        "Data augmentation - used in combination with our other improvements - seems to help a lot! Our test error is now comfortably under 3%."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
