{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rM_e9o7kUck"
      },
      "source": [
        "### Model with dropout (and ReLU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97kkvz26lhEg"
      },
      "source": [
        "Another tools to combat overfitting is regularization. To regularize the network, we add a dropout layer with 0.25 probability just before the layer with the largest number of parameters (H3). \n",
        "\n",
        "Because dropout zeros some activations, it doesnâ€™t make as much sense to use it with tanh activations that map outputs to the range -1 to 1. So, we also change the activations throughout the hidden layers from tanh to ReLU.\n",
        "\n",
        "Finally, we increase the number of training epochs again to 80, since dropout introduces some more noise in the training process. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJqRxqAdelLg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "torch.set_num_threads(2) # for performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZhoqyy9lntD"
      },
      "outputs": [],
      "source": [
        "# The following code is adapted from \"modern.py\" in the repository.\n",
        "\n",
        "class ModernDropoutNet(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # initialization as described in the paper to my best ability, but it doesn't look right...\n",
        "        winit = lambda fan_in, *shape: (torch.rand(*shape) - 0.5) * 2 * 2.4 / fan_in**0.5\n",
        "        macs = 0 # keep track of MACs (multiply accumulates)\n",
        "        acts = 0 # keep track of number of activations\n",
        "\n",
        "        # H1 layer parameters and their initialization\n",
        "        self.H1w = nn.Parameter(winit(5*5*1, 12, 1, 5, 5))\n",
        "        self.H1b = nn.Parameter(torch.zeros(12, 8, 8)) # presumably init to zero for biases\n",
        "        macs += (5*5*1) * (8*8) * 12\n",
        "        acts += (8*8) * 12\n",
        "\n",
        "        # H2 layer parameters and their initialization\n",
        "        \"\"\"\n",
        "        H2 neurons all connect to only 8 of the 12 input planes, with an unspecified pattern\n",
        "        I am going to assume the most sensible block pattern where 4 planes at a time connect\n",
        "        to differently overlapping groups of 8/12 input planes. We will implement this with 3\n",
        "        separate convolutions that we concatenate the results of.\n",
        "        \"\"\"\n",
        "        self.H2w = nn.Parameter(winit(5*5*8, 12, 8, 5, 5))\n",
        "        self.H2b = nn.Parameter(torch.zeros(12, 4, 4)) # presumably init to zero for biases\n",
        "        macs += (5*5*8) * (4*4) * 12\n",
        "        acts += (4*4) * 12\n",
        "\n",
        "        # H3 is a fully connected layer\n",
        "        self.H3w = nn.Parameter(winit(4*4*12, 4*4*12, 30))\n",
        "        self.H3b = nn.Parameter(torch.zeros(30))\n",
        "        macs += (4*4*12) * 30\n",
        "        acts += 30\n",
        "\n",
        "        # output layer is also fully connected layer\n",
        "        self.outw = nn.Parameter(winit(30, 30, 10))\n",
        "        self.outb = nn.Parameter(torch.zeros(10))\n",
        "        macs += 30 * 10\n",
        "        acts += 10\n",
        "\n",
        "        self.macs = macs\n",
        "        self.acts = acts\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # poor man's data augmentation by 1 pixel along x/y directions\n",
        "        if self.training:\n",
        "            shift_x, shift_y = np.random.randint(-1, 2, size=2)\n",
        "            x = torch.roll(x, (shift_x, shift_y), (2, 3))\n",
        "\n",
        "        # x has shape (1, 1, 16, 16)\n",
        "        x = F.pad(x, (2, 2, 2, 2), 'constant', -1.0) # pad by two using constant -1 for background\n",
        "        x = F.conv2d(x, self.H1w, stride=2) + self.H1b\n",
        "        x = torch.relu(x) # Note: ReLU activation!\n",
        "\n",
        "        # x is now shape (1, 12, 8, 8)\n",
        "        x = F.pad(x, (2, 2, 2, 2), 'constant', -1.0) # pad by two using constant -1 for background\n",
        "        slice1 = F.conv2d(x[:, 0:8], self.H2w[0:4], stride=2) # first 4 planes look at first 8 input planes\n",
        "        slice2 = F.conv2d(x[:, 4:12], self.H2w[4:8], stride=2) # next 4 planes look at last 8 input planes\n",
        "        slice3 = F.conv2d(torch.cat((x[:, 0:4], x[:, 8:12]), dim=1), self.H2w[8:12], stride=2) # last 4 planes are cross\n",
        "        x = torch.cat((slice1, slice2, slice3), dim=1) + self.H2b\n",
        "        x = torch.relu(x) # Note: ReLU activation!\n",
        "\n",
        "        # x is now shape (1, 12, 4, 4)\n",
        "        x = x.flatten(start_dim=1) # (1, 12*4*4)\n",
        "        x = x @ self.H3w + self.H3b\n",
        "        x = torch.relu(x) # Note: ReLU activation!\n",
        "        x = F.dropout(x, p=0.25, training=self.training) # Note: dropout layer!\n",
        "\n",
        "\n",
        "        # x is now shape (1, 30)\n",
        "        x = x @ self.outw + self.outb\n",
        "\n",
        "         # x is finally shape (1, 10)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHH4okrikXvq",
        "outputId": "0bc1a154-86c4-42f7-c155-feecc59d2c52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model stats:\n",
            "# params:       9760\n",
            "# MACs:         63660\n",
            "# activations:  1000\n",
            "1\n",
            "eval: split train. loss 5.223247e-01. error 13.59%. misses: 991\n",
            "eval: split test . loss 5.147547e-01. error 13.25%. misses: 266\n",
            "2\n",
            "eval: split train. loss 3.238891e-01. error 8.53%. misses: 622\n",
            "eval: split test . loss 3.289565e-01. error 8.92%. misses: 179\n",
            "3\n",
            "eval: split train. loss 2.529212e-01. error 6.75%. misses: 491\n",
            "eval: split test . loss 2.521613e-01. error 6.93%. misses: 139\n",
            "4\n",
            "eval: split train. loss 1.924911e-01. error 5.72%. misses: 417\n",
            "eval: split test . loss 1.910119e-01. error 5.48%. misses: 109\n",
            "5\n",
            "eval: split train. loss 1.814650e-01. error 5.50%. misses: 401\n",
            "eval: split test . loss 1.837532e-01. error 5.58%. misses: 112\n",
            "6\n",
            "eval: split train. loss 1.666141e-01. error 5.27%. misses: 384\n",
            "eval: split test . loss 1.673810e-01. error 5.13%. misses: 102\n",
            "7\n",
            "eval: split train. loss 1.402711e-01. error 4.54%. misses: 331\n",
            "eval: split test . loss 1.433306e-01. error 4.43%. misses: 89\n",
            "8\n",
            "eval: split train. loss 1.365579e-01. error 4.25%. misses: 309\n",
            "eval: split test . loss 1.373578e-01. error 4.24%. misses: 84\n",
            "9\n",
            "eval: split train. loss 1.249110e-01. error 3.95%. misses: 287\n",
            "eval: split test . loss 1.320812e-01. error 4.33%. misses: 87\n",
            "10\n",
            "eval: split train. loss 1.134094e-01. error 3.42%. misses: 249\n",
            "eval: split test . loss 1.182142e-01. error 3.74%. misses: 74\n",
            "11\n",
            "eval: split train. loss 1.168040e-01. error 3.83%. misses: 279\n",
            "eval: split test . loss 1.203498e-01. error 3.59%. misses: 72\n",
            "12\n",
            "eval: split train. loss 1.058118e-01. error 3.35%. misses: 244\n",
            "eval: split test . loss 1.125635e-01. error 3.34%. misses: 66\n",
            "13\n",
            "eval: split train. loss 1.101326e-01. error 3.33%. misses: 242\n",
            "eval: split test . loss 1.211635e-01. error 4.09%. misses: 82\n",
            "14\n",
            "eval: split train. loss 9.793758e-02. error 2.80%. misses: 204\n",
            "eval: split test . loss 1.007514e-01. error 3.04%. misses: 61\n",
            "15\n",
            "eval: split train. loss 1.014829e-01. error 3.17%. misses: 231\n",
            "eval: split test . loss 1.086887e-01. error 3.49%. misses: 69\n",
            "16\n",
            "eval: split train. loss 1.106245e-01. error 3.61%. misses: 263\n",
            "eval: split test . loss 1.268642e-01. error 4.33%. misses: 87\n",
            "17\n",
            "eval: split train. loss 8.915594e-02. error 2.84%. misses: 207\n",
            "eval: split test . loss 1.010848e-01. error 3.24%. misses: 64\n",
            "18\n",
            "eval: split train. loss 8.456316e-02. error 2.63%. misses: 192\n",
            "eval: split test . loss 9.889623e-02. error 3.04%. misses: 61\n",
            "19\n",
            "eval: split train. loss 8.955027e-02. error 2.88%. misses: 209\n",
            "eval: split test . loss 1.047495e-01. error 3.34%. misses: 66\n",
            "20\n",
            "eval: split train. loss 9.096973e-02. error 2.89%. misses: 211\n",
            "eval: split test . loss 9.660224e-02. error 2.84%. misses: 57\n",
            "21\n",
            "eval: split train. loss 9.989870e-02. error 3.32%. misses: 242\n",
            "eval: split test . loss 1.095374e-01. error 3.44%. misses: 69\n",
            "22\n",
            "eval: split train. loss 8.133950e-02. error 2.65%. misses: 192\n",
            "eval: split test . loss 9.848556e-02. error 3.14%. misses: 62\n",
            "23\n",
            "eval: split train. loss 8.754805e-02. error 2.58%. misses: 188\n",
            "eval: split test . loss 9.644570e-02. error 2.84%. misses: 57\n",
            "24\n",
            "eval: split train. loss 7.531853e-02. error 2.46%. misses: 178\n",
            "eval: split test . loss 8.441807e-02. error 2.29%. misses: 46\n",
            "25\n",
            "eval: split train. loss 8.178604e-02. error 2.62%. misses: 191\n",
            "eval: split test . loss 9.372281e-02. error 2.89%. misses: 57\n",
            "26\n",
            "eval: split train. loss 8.119241e-02. error 2.66%. misses: 193\n",
            "eval: split test . loss 8.070310e-02. error 2.74%. misses: 54\n",
            "27\n",
            "eval: split train. loss 7.476155e-02. error 2.24%. misses: 162\n",
            "eval: split test . loss 9.376104e-02. error 3.04%. misses: 61\n",
            "28\n",
            "eval: split train. loss 7.216480e-02. error 2.29%. misses: 166\n",
            "eval: split test . loss 9.313789e-02. error 2.94%. misses: 58\n",
            "29\n",
            "eval: split train. loss 7.018799e-02. error 2.04%. misses: 149\n",
            "eval: split test . loss 8.587180e-02. error 2.69%. misses: 53\n",
            "30\n",
            "eval: split train. loss 6.577878e-02. error 2.04%. misses: 149\n",
            "eval: split test . loss 7.740857e-02. error 2.44%. misses: 48\n",
            "31\n",
            "eval: split train. loss 6.588963e-02. error 2.22%. misses: 161\n",
            "eval: split test . loss 7.523805e-02. error 2.49%. misses: 49\n",
            "32\n",
            "eval: split train. loss 6.469564e-02. error 2.02%. misses: 146\n",
            "eval: split test . loss 7.887840e-02. error 2.34%. misses: 47\n",
            "33\n",
            "eval: split train. loss 6.657082e-02. error 2.06%. misses: 149\n",
            "eval: split test . loss 8.518770e-02. error 2.79%. misses: 56\n",
            "34\n",
            "eval: split train. loss 6.934658e-02. error 2.25%. misses: 164\n",
            "eval: split test . loss 7.846080e-02. error 2.54%. misses: 51\n",
            "35\n",
            "eval: split train. loss 6.879612e-02. error 2.21%. misses: 161\n",
            "eval: split test . loss 7.792255e-02. error 2.24%. misses: 44\n",
            "36\n",
            "eval: split train. loss 6.565613e-02. error 2.02%. misses: 146\n",
            "eval: split test . loss 7.835120e-02. error 2.29%. misses: 46\n",
            "37\n",
            "eval: split train. loss 6.221177e-02. error 1.93%. misses: 141\n",
            "eval: split test . loss 7.628819e-02. error 2.29%. misses: 46\n",
            "38\n",
            "eval: split train. loss 7.065709e-02. error 2.44%. misses: 177\n",
            "eval: split test . loss 7.877579e-02. error 2.34%. misses: 47\n",
            "39\n",
            "eval: split train. loss 6.042336e-02. error 1.84%. misses: 134\n",
            "eval: split test . loss 7.865900e-02. error 2.39%. misses: 47\n",
            "40\n",
            "eval: split train. loss 5.811433e-02. error 1.88%. misses: 137\n",
            "eval: split test . loss 7.182295e-02. error 2.39%. misses: 47\n",
            "41\n",
            "eval: split train. loss 6.778783e-02. error 2.37%. misses: 173\n",
            "eval: split test . loss 8.666728e-02. error 3.14%. misses: 62\n",
            "42\n",
            "eval: split train. loss 5.899702e-02. error 1.89%. misses: 138\n",
            "eval: split test . loss 7.252682e-02. error 2.34%. misses: 47\n",
            "43\n",
            "eval: split train. loss 6.142176e-02. error 2.00%. misses: 145\n",
            "eval: split test . loss 7.867751e-02. error 2.49%. misses: 49\n",
            "44\n",
            "eval: split train. loss 5.371898e-02. error 1.70%. misses: 123\n",
            "eval: split test . loss 7.723532e-02. error 2.24%. misses: 44\n",
            "45\n",
            "eval: split train. loss 5.887199e-02. error 1.87%. misses: 135\n",
            "eval: split test . loss 7.906169e-02. error 2.44%. misses: 48\n",
            "46\n",
            "eval: split train. loss 5.735618e-02. error 1.81%. misses: 131\n",
            "eval: split test . loss 7.473596e-02. error 2.44%. misses: 48\n",
            "47\n",
            "eval: split train. loss 5.248378e-02. error 1.49%. misses: 109\n",
            "eval: split test . loss 6.969590e-02. error 2.09%. misses: 42\n",
            "48\n",
            "eval: split train. loss 5.893116e-02. error 1.77%. misses: 129\n",
            "eval: split test . loss 8.039889e-02. error 2.84%. misses: 57\n",
            "49\n",
            "eval: split train. loss 5.989001e-02. error 1.88%. misses: 137\n",
            "eval: split test . loss 8.470745e-02. error 2.79%. misses: 56\n",
            "50\n",
            "eval: split train. loss 5.442727e-02. error 1.65%. misses: 119\n",
            "eval: split test . loss 8.003248e-02. error 2.79%. misses: 56\n",
            "51\n",
            "eval: split train. loss 5.671265e-02. error 1.71%. misses: 125\n",
            "eval: split test . loss 8.537056e-02. error 2.69%. misses: 53\n",
            "52\n",
            "eval: split train. loss 4.895335e-02. error 1.44%. misses: 104\n",
            "eval: split test . loss 7.351615e-02. error 2.29%. misses: 46\n",
            "53\n",
            "eval: split train. loss 5.122150e-02. error 1.66%. misses: 121\n",
            "eval: split test . loss 6.803998e-02. error 2.14%. misses: 43\n",
            "54\n",
            "eval: split train. loss 4.873757e-02. error 1.62%. misses: 118\n",
            "eval: split test . loss 6.716364e-02. error 2.09%. misses: 42\n",
            "55\n",
            "eval: split train. loss 5.735023e-02. error 1.81%. misses: 131\n",
            "eval: split test . loss 8.123089e-02. error 2.49%. misses: 49\n",
            "56\n",
            "eval: split train. loss 4.881221e-02. error 1.67%. misses: 122\n",
            "eval: split test . loss 6.735352e-02. error 2.29%. misses: 46\n",
            "57\n",
            "eval: split train. loss 5.675887e-02. error 1.87%. misses: 135\n",
            "eval: split test . loss 7.805803e-02. error 2.64%. misses: 52\n",
            "58\n",
            "eval: split train. loss 5.146070e-02. error 1.77%. misses: 129\n",
            "eval: split test . loss 6.974848e-02. error 2.09%. misses: 42\n",
            "59\n",
            "eval: split train. loss 4.592435e-02. error 1.43%. misses: 104\n",
            "eval: split test . loss 6.560948e-02. error 2.09%. misses: 42\n",
            "60\n",
            "eval: split train. loss 4.786340e-02. error 1.54%. misses: 111\n",
            "eval: split test . loss 6.433708e-02. error 1.94%. misses: 38\n",
            "61\n",
            "eval: split train. loss 4.798522e-02. error 1.65%. misses: 119\n",
            "eval: split test . loss 6.778712e-02. error 2.39%. misses: 47\n",
            "62\n",
            "eval: split train. loss 4.751759e-02. error 1.39%. misses: 100\n",
            "eval: split test . loss 6.549032e-02. error 1.89%. misses: 38\n",
            "63\n",
            "eval: split train. loss 4.841748e-02. error 1.45%. misses: 106\n",
            "eval: split test . loss 7.229067e-02. error 2.14%. misses: 43\n",
            "64\n",
            "eval: split train. loss 4.649686e-02. error 1.54%. misses: 111\n",
            "eval: split test . loss 7.065659e-02. error 2.14%. misses: 43\n",
            "65\n",
            "eval: split train. loss 4.684654e-02. error 1.39%. misses: 100\n",
            "eval: split test . loss 6.878091e-02. error 2.29%. misses: 46\n",
            "66\n",
            "eval: split train. loss 4.544485e-02. error 1.39%. misses: 100\n",
            "eval: split test . loss 7.179146e-02. error 2.24%. misses: 44\n",
            "67\n",
            "eval: split train. loss 4.926059e-02. error 1.54%. misses: 111\n",
            "eval: split test . loss 7.378801e-02. error 2.54%. misses: 51\n",
            "68\n",
            "eval: split train. loss 4.373996e-02. error 1.60%. misses: 117\n",
            "eval: split test . loss 6.775968e-02. error 2.19%. misses: 43\n",
            "69\n",
            "eval: split train. loss 4.506001e-02. error 1.45%. misses: 106\n",
            "eval: split test . loss 6.757440e-02. error 2.29%. misses: 46\n",
            "70\n",
            "eval: split train. loss 5.111226e-02. error 1.67%. misses: 122\n",
            "eval: split test . loss 7.468051e-02. error 2.19%. misses: 43\n",
            "71\n",
            "eval: split train. loss 4.644617e-02. error 1.43%. misses: 104\n",
            "eval: split test . loss 6.787570e-02. error 2.24%. misses: 44\n",
            "72\n",
            "eval: split train. loss 4.204433e-02. error 1.28%. misses: 92\n",
            "eval: split test . loss 6.072659e-02. error 1.84%. misses: 37\n",
            "73\n",
            "eval: split train. loss 4.703084e-02. error 1.45%. misses: 106\n",
            "eval: split test . loss 7.324030e-02. error 2.09%. misses: 42\n",
            "74\n",
            "eval: split train. loss 4.614950e-02. error 1.39%. misses: 100\n",
            "eval: split test . loss 7.064499e-02. error 2.14%. misses: 43\n",
            "75\n",
            "eval: split train. loss 4.277621e-02. error 1.29%. misses: 94\n",
            "eval: split test . loss 6.572228e-02. error 2.14%. misses: 43\n",
            "76\n",
            "eval: split train. loss 4.225215e-02. error 1.41%. misses: 102\n",
            "eval: split test . loss 6.958887e-02. error 2.04%. misses: 41\n",
            "77\n",
            "eval: split train. loss 3.904821e-02. error 1.25%. misses: 90\n",
            "eval: split test . loss 6.618915e-02. error 2.14%. misses: 43\n",
            "78\n",
            "eval: split train. loss 4.348967e-02. error 1.49%. misses: 109\n",
            "eval: split test . loss 6.538693e-02. error 2.04%. misses: 41\n",
            "79\n",
            "eval: split train. loss 4.004970e-02. error 1.43%. misses: 104\n",
            "eval: split test . loss 6.410659e-02. error 2.09%. misses: 42\n",
            "80\n",
            "eval: split train. loss 3.975540e-02. error 1.43%. misses: 104\n",
            "eval: split test . loss 6.317432e-02. error 2.09%. misses: 42\n"
          ]
        }
      ],
      "source": [
        "# The following code is adapted from \"modern.py\" in the repository.\n",
        "\n",
        "learning_rate = 3e-4\n",
        "\n",
        "# init rng\n",
        "torch.manual_seed(1337)\n",
        "np.random.seed(1337)\n",
        "torch.use_deterministic_algorithms(True)\n",
        "\n",
        "# init a model\n",
        "model = ModernDropoutNet()\n",
        "print(\"model stats:\")\n",
        "print(\"# params:      \", sum(p.numel() for p in model.parameters())) # in paper total is 9,760\n",
        "print(\"# MACs:        \", model.macs)\n",
        "print(\"# activations: \", model.acts)\n",
        "\n",
        "# init data\n",
        "Xtr, Ytr = torch.load('train1989.pt')\n",
        "Xte, Yte = torch.load('test1989.pt')\n",
        "\n",
        "# init optimizer\n",
        "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "def eval_split(split):\n",
        "    # eval the full train/test set, batched implementation for efficiency\n",
        "    model.eval()\n",
        "    X, Y = (Xtr, Ytr) if split == 'train' else (Xte, Yte)\n",
        "    Yhat = model(X)\n",
        "    loss = F.cross_entropy(Yhat, Y.argmax(dim=1))\n",
        "    err = torch.mean((Y.argmax(dim=1) != Yhat.argmax(dim=1)).float())\n",
        "    print(f\"eval: split {split:5s}. loss {loss.item():e}. error {err.item()*100:.2f}%. misses: {int(err.item()*Y.size(0))}\")\n",
        "  \n",
        "# train\n",
        "for pass_num in range(80):\n",
        "\n",
        "    # learning rate decay\n",
        "    alpha = pass_num / 79\n",
        "    for g in optimizer.param_groups:\n",
        "        g['lr'] = (1 - alpha) * learning_rate + alpha * (learning_rate / 3)\n",
        "\n",
        "    # perform one epoch of training\n",
        "    model.train()\n",
        "    for step_num in range(Xtr.size(0)):\n",
        "\n",
        "        # fetch a single example into a batch of 1\n",
        "        x, y = Xtr[[step_num]], Ytr[[step_num]]\n",
        "\n",
        "        # forward the model and the loss\n",
        "        yhat = model(x)\n",
        "        loss = F.cross_entropy(yhat, y.argmax(dim=1))\n",
        "\n",
        "        # calculate the gradient and update the parameters\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # after epoch epoch evaluate the train and test error / metrics\n",
        "    print(pass_num + 1)\n",
        "    eval_split('train')\n",
        "    eval_split('test')\n",
        "\n",
        "# save final model to file\n",
        "torch.save(model.state_dict(), 'dropout_model.pt')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
